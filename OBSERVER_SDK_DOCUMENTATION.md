# TigerHill Observer SDK - å®Œæ•´æ–‡æ¡£

## ğŸ“– ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [API å‚è€ƒ](#api-å‚è€ƒ)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## æ¦‚è¿°

TigerHill Observer SDK æ˜¯ä¸€ä¸ª**æ— ä¾µå…¥å¼çš„ LLM è°ƒè¯•å·¥å…·**ï¼Œç”¨äºåœ¨å¼€å‘é˜¶æ®µæ•è·ã€åˆ†æå’Œä¼˜åŒ– prompt å’Œå“åº”ã€‚

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

1. **Prompt æ•è·** - è‡ªåŠ¨è®°å½•æ‰€æœ‰ LLM è¯·æ±‚å’Œå“åº”
2. **è‡ªåŠ¨åˆ†æ** - Token ä½¿ç”¨ã€è´¨é‡è¯„ä¼°ã€æ€§èƒ½åˆ†æ
3. **éšç§ä¿æŠ¤** - è‡ªåŠ¨è„±æ•æ•æ„Ÿä¿¡æ¯ï¼ˆAPI keysã€é‚®ç®±ç­‰ï¼‰
4. **TraceStore é›†æˆ** - æ— ç¼è½¬æ¢ä¸ºæµ‹è¯•ç”¨ä¾‹
5. **è·¨è¯­è¨€æ”¯æŒ** - Python å’Œ Node.js

### ğŸ—ï¸ æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Your Application Code                 â”‚
â”‚                                                 â”‚
â”‚  model.generate_content("prompt")              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         TigerHill Observer SDK                  â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Wrapper    â”‚â†’ â”‚   Capture    â”‚           â”‚
â”‚  â”‚   (é€æ˜åŒ…è£…)  â”‚  â”‚   (æ•°æ®æ•è·)  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                            â”‚                    â”‚
â”‚                            â–¼                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Sanitizer   â”‚â†’ â”‚   Storage    â”‚           â”‚
â”‚  â”‚  (è„±æ•å¤„ç†)   â”‚  â”‚   (æŒä¹…åŒ–)    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Google Generative AI                  â”‚
â”‚                                                 â”‚
â”‚  Gemini API â†’ Response                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”‘ è®¾è®¡åŸåˆ™

- **æ— ä¾µå…¥**: é€šè¿‡åŒ…è£…å™¨æ¨¡å¼ï¼Œä¸ä¿®æ”¹åŸå§‹ä»£ç 
- **é€æ˜åŒ–**: è‡ªåŠ¨æ•è·ï¼Œå¼€å‘è€…æ— æ„ŸçŸ¥
- **å®‰å…¨æ€§**: è‡ªåŠ¨è„±æ•ï¼Œä¿æŠ¤éšç§
- **å¯æ‰©å±•**: æ”¯æŒè‡ªå®šä¹‰å›è°ƒå’Œè§„åˆ™

---

## å¿«é€Ÿå¼€å§‹

### Python - 5 åˆ†é’Ÿä¸Šæ‰‹

#### 1. å®‰è£…ä¾èµ–

```bash
pip install google-generativeai
```

#### 2. åŸºç¡€ä½¿ç”¨

```python
import os
from tigerhill.observer import PromptCapture, wrap_python_model
from tigerhill.observer.python_observer import create_observer_callback
import google.generativeai as genai

# é…ç½® API
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# åˆ›å»ºæ•è·å™¨
capture = PromptCapture(storage_path="./captures")
capture_id = capture.start_capture("my_agent")

# åˆ›å»ºå›è°ƒ
callback = create_observer_callback(capture, capture_id)

# åŒ…è£…æ¨¡å‹
WrappedModel = wrap_python_model(
    genai.GenerativeModel,
    capture_callback=callback
)

# ä½¿ç”¨åŒ…è£…åçš„æ¨¡å‹ï¼ˆå®Œå…¨é€æ˜ï¼‰
model = WrappedModel("gemini-pro")
response = model.generate_content("Hello!")

# ç»“æŸæ•è·
result = capture.end_capture(capture_id)
print(f"Captured {result['statistics']['total_requests']} requests")
```

#### 3. è¿è¡Œåˆ†æ

```python
from tigerhill.observer import PromptAnalyzer

analyzer = PromptAnalyzer(result)
report = analyzer.analyze_all()
analyzer.print_report(report)
```

### Node.js - 5 åˆ†é’Ÿä¸Šæ‰‹

#### 1. å®‰è£…ä¾èµ–

```bash
npm install @google/generative-ai
```

#### 2. åŸºç¡€ä½¿ç”¨

```javascript
const { GoogleGenerativeAI } = require('@google/generative-ai');
const { wrapGenerativeModel } = require('./tigerhill/observer/node_observer');

// åŒ…è£…æ¨¡å‹ç±»
const WrappedModel = wrapGenerativeModel(GoogleGenerativeAI, {
    onRequest: (data) => console.log('Request:', data),
    onResponse: (data) => console.log('Response:', data),
    autoExport: true,
    exportPath: './captures'
});

// ä½¿ç”¨
const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
const model = genAI.getGenerativeModel({ model: 'gemini-pro' });

const result = await model.generateContent('Hello!');
```

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1. Capture (æ•è·)

æ•è·æ˜¯ Observer SDK çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œè®°å½• LLM äº¤äº’çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚

#### Capture å¯¹è±¡

```python
capture = PromptCapture(
    storage_path="./captures",    # å­˜å‚¨è·¯å¾„
    auto_save=True,                # è‡ªåŠ¨ä¿å­˜
    redact_patterns=[]             # è‡ªå®šä¹‰è„±æ•è§„åˆ™
)
```

#### Capture Session

æ¯ä¸ª capture session åŒ…å«ï¼š

- **capture_id**: å”¯ä¸€æ ‡è¯†ç¬¦
- **agent_name**: Agent åç§°
- **metadata**: è‡ªå®šä¹‰å…ƒæ•°æ®
- **requests**: è¯·æ±‚åˆ—è¡¨
- **responses**: å“åº”åˆ—è¡¨
- **tool_calls**: å·¥å…·è°ƒç”¨åˆ—è¡¨
- **statistics**: ç»Ÿè®¡ä¿¡æ¯

#### ç”Ÿå‘½å‘¨æœŸ

```python
# 1. å¼€å§‹æ•è·
capture_id = capture.start_capture("agent_name")

# 2. æ•è·è¯·æ±‚
capture.capture_request(capture_id, request_data)

# 3. æ•è·å“åº”
capture.capture_response(capture_id, response_data)

# 4. ç»“æŸæ•è·
result = capture.end_capture(capture_id)
```

### 2. Observer (è§‚å¯Ÿå™¨)

Observer æ˜¯é€æ˜çš„åŒ…è£…å™¨ï¼Œè‡ªåŠ¨æ•è· LLM SDK çš„è°ƒç”¨ã€‚

#### Python Observer

```python
WrappedModel = wrap_python_model(
    model_class,              # è¦åŒ…è£…çš„æ¨¡å‹ç±»
    capture_callback,         # æ•è·å›è°ƒå‡½æ•°
    capture_response=True     # æ˜¯å¦æ•è·å“åº”
)
```

**æ”¯æŒçš„æ–¹æ³•**:
- `generate_content()` - åŒæ­¥ç”Ÿæˆ
- `generate_content_async()` - å¼‚æ­¥ç”Ÿæˆ

**è‡ªåŠ¨æå–**:
- Prompt (æ–‡æœ¬/å¤šè½®å¯¹è¯)
- System prompt
- Generation config
- Tools
- Safety settings
- Usage metadata
- Tool calls

#### Node.js Observer

```javascript
WrappedModel = wrapGenerativeModel(ModelClass, {
    onRequest: callback,       // è¯·æ±‚å›è°ƒ
    onResponse: callback,      // å“åº”å›è°ƒ
    captureEndpoint: url,      // è¿œç¨‹ç«¯ç‚¹
    autoExport: true,          // è‡ªåŠ¨å¯¼å‡º
    exportPath: path           // å¯¼å‡ºè·¯å¾„
});
```

**æ”¯æŒçš„æ–¹æ³•**:
- `generateContent()` - å¸¸è§„ç”Ÿæˆ
- `generateContentStream()` - æµå¼ç”Ÿæˆ

### 3. Analyzer (åˆ†æå™¨)

Analyzer æä¾›è‡ªåŠ¨åˆ†æå’Œä¼˜åŒ–å»ºè®®ã€‚

#### åˆ›å»ºåˆ†æå™¨

```python
analyzer = PromptAnalyzer(capture_data)
```

æ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š
- `PromptCapture` å®ä¾‹
- å•ä¸ªæ•è·æ•°æ®å­—å…¸
- æ•è·æ•°æ®åˆ—è¡¨

#### åˆ†æç»´åº¦

1. **Token Analysis** - Token ä½¿ç”¨åˆ†æ
   ```python
   token_report = analyzer.analyze_tokens()
   # è¿”å›ï¼štotal, avg, efficiency_ratio, max, min
   ```

2. **Prompt Quality** - è´¨é‡è¯„ä¼°
   ```python
   quality_report = analyzer.analyze_prompt_quality()
   # è¿”å›ï¼šclarity_score, system_prompt_ratio, issues
   ```

3. **Performance** - æ€§èƒ½åˆ†æ
   ```python
   perf_report = analyzer.analyze_performance()
   # è¿”å›ï¼šavg_duration, max, min, total
   ```

4. **Tool Usage** - å·¥å…·ä½¿ç”¨
   ```python
   tool_report = analyzer.analyze_tool_usage()
   # è¿”å›ï¼šdefined, called, unused, most_used
   ```

5. **Recommendations** - ä¼˜åŒ–å»ºè®®
   ```python
   recommendations = analyzer.generate_recommendations()
   # è¿”å›ï¼šcategory, severity, title, suggestion
   ```

#### å®Œæ•´åˆ†æ

```python
report = analyzer.analyze_all()
analyzer.print_report(report)
```

### 4. Sanitization (è„±æ•)

è‡ªåŠ¨ä¿æŠ¤æ•æ„Ÿä¿¡æ¯ã€‚

#### é»˜è®¤è§„åˆ™

| ç±»å‹ | æ­£åˆ™è¡¨è¾¾å¼ | æ›¿æ¢å€¼ |
|------|-----------|--------|
| API Keys | `(AIza\|sk-)[0-9A-Za-z-_]{20,}` | `<REDACTED_API_KEY>` |
| Emails | `[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}` | `<REDACTED_EMAIL>` |
| Credit Cards | `\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}` | `<REDACTED_CARD>` |

#### è‡ªå®šä¹‰è§„åˆ™

```python
custom_patterns = [
    {
        "pattern": r"SECRET-\d{6}",
        "replacement": "<SECRET>"
    },
    {
        "pattern": r"TOKEN_[A-Z0-9]{32}",
        "replacement": "<TOKEN>"
    }
]

capture = PromptCapture(redact_patterns=custom_patterns)
```

### 5. TraceStore Integration

å°†æ•è·æ•°æ®å¯¼å‡ºä¸ºæµ‹è¯•ç”¨ä¾‹ã€‚

```python
trace_id = capture.export_to_trace_store(
    capture_id=capture_id,
    trace_store=trace_store,
    agent_name="my_agent"
)
```

**å¯¼å‡ºå†…å®¹**:
- `prompt_request` äº‹ä»¶
- `model_response` äº‹ä»¶
- `tool_call` äº‹ä»¶
- `statistics` äº‹ä»¶

---

## API å‚è€ƒ

### PromptCapture

#### æ„é€ å‡½æ•°

```python
PromptCapture(
    storage_path: str = "./prompt_captures",
    auto_save: bool = True,
    redact_patterns: Optional[List[Dict[str, str]]] = None
)
```

#### æ–¹æ³•

##### `start_capture(agent_name, metadata=None) -> str`

å¼€å§‹æ–°çš„æ•è·ä¼šè¯ã€‚

**å‚æ•°**:
- `agent_name` (str): Agent åç§°
- `metadata` (dict, optional): è‡ªå®šä¹‰å…ƒæ•°æ®

**è¿”å›**: capture_id (str)

**ç¤ºä¾‹**:
```python
capture_id = capture.start_capture(
    "code_assistant",
    metadata={"version": "1.0", "task": "refactor"}
)
```

##### `capture_request(capture_id, request_data) -> None`

æ•è·è¯·æ±‚æ•°æ®ã€‚

**å‚æ•°**:
- `capture_id` (str): æ•è·ä¼šè¯ ID
- `request_data` (dict): è¯·æ±‚æ•°æ®
  - `model` (str): æ¨¡å‹åç§°
  - `prompt` (str|list): Prompt å†…å®¹
  - `system_prompt` (str, optional): ç³»ç»Ÿ prompt
  - `temperature` (float, optional): æ¸©åº¦å‚æ•°
  - `tools` (list, optional): å·¥å…·åˆ—è¡¨

**ç¤ºä¾‹**:
```python
capture.capture_request(capture_id, {
    "model": "gemini-pro",
    "prompt": "Write a function...",
    "system_prompt": "You are a coding assistant",
    "temperature": 0.7,
    "tools": [{"name": "search"}]
})
```

##### `capture_response(capture_id, response_data) -> None`

æ•è·å“åº”æ•°æ®ã€‚

**å‚æ•°**:
- `capture_id` (str): æ•è·ä¼šè¯ ID
- `response_data` (dict): å“åº”æ•°æ®
  - `text` (str): å“åº”æ–‡æœ¬
  - `finish_reason` (str): å®ŒæˆåŸå› 
  - `usage` (dict): Token ä½¿ç”¨
  - `tool_calls` (list, optional): å·¥å…·è°ƒç”¨

**ç¤ºä¾‹**:
```python
capture.capture_response(capture_id, {
    "text": "Here is the function...",
    "finish_reason": "stop",
    "usage": {
        "prompt_tokens": 50,
        "completion_tokens": 100,
        "total_tokens": 150
    }
})
```

##### `end_capture(capture_id) -> Dict[str, Any]`

ç»“æŸæ•è·ä¼šè¯å¹¶è¿”å›å®Œæ•´æ•°æ®ã€‚

**å‚æ•°**:
- `capture_id` (str): æ•è·ä¼šè¯ ID

**è¿”å›**: å®Œæ•´çš„æ•è·æ•°æ®ï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯

**ç¤ºä¾‹**:
```python
result = capture.end_capture(capture_id)
print(f"Duration: {result['duration']:.2f}s")
print(f"Total tokens: {result['statistics']['total_tokens']}")
```

##### `get_capture(capture_id) -> Optional[Dict[str, Any]]`

è·å–æŒ‡å®šçš„æ•è·æ•°æ®ã€‚

##### `list_captures(agent_name=None, status=None) -> List[Dict[str, Any]]`

åˆ—å‡ºæ•è·ä¼šè¯ã€‚

**å‚æ•°**:
- `agent_name` (str, optional): æŒ‰ agent è¿‡æ»¤
- `status` (str, optional): æŒ‰çŠ¶æ€è¿‡æ»¤ (active/completed)

##### `load_capture(capture_id) -> Optional[Dict[str, Any]]`

ä»æ–‡ä»¶åŠ è½½æ•è·æ•°æ®ã€‚

##### `export_to_trace_store(capture_id, trace_store, agent_name=None) -> str`

å¯¼å‡ºåˆ° TraceStoreã€‚

**è¿”å›**: trace_id

---

### PromptAnalyzer

#### æ„é€ å‡½æ•°

```python
PromptAnalyzer(capture)
```

**å‚æ•°**:
- `capture`: PromptCapture å®ä¾‹ã€å­—å…¸æˆ–åˆ—è¡¨

#### æ–¹æ³•

##### `analyze_all() -> Dict[str, Any]`

æ‰§è¡Œå®Œæ•´åˆ†æï¼Œè¿”å›å®Œæ•´æŠ¥å‘Šã€‚

**è¿”å›**:
```python
{
    "summary": {...},
    "token_analysis": {...},
    "prompt_quality": {...},
    "performance": {...},
    "tool_usage": {...},
    "recommendations": [...]
}
```

##### `get_summary() -> Dict[str, Any]`

è·å–æ‘˜è¦ä¿¡æ¯ã€‚

**è¿”å›**:
```python
{
    "total_captures": 1,
    "total_requests": 2,
    "total_responses": 2,
    "unique_agents": 1,
    "unique_models": 1,
    "agents": ["agent1"],
    "models": ["gemini-pro"]
}
```

##### `analyze_tokens() -> Dict[str, Any]`

åˆ†æ Token ä½¿ç”¨ã€‚

**è¿”å›**:
```python
{
    "total_tokens": 1000,
    "total_prompt_tokens": 400,
    "total_completion_tokens": 600,
    "avg_tokens_per_request": 500,
    "token_efficiency_ratio": 1.5,
    "max_tokens": 700,
    "min_tokens": 300
}
```

##### `analyze_prompt_quality() -> Dict[str, Any]`

åˆ†æ Prompt è´¨é‡ã€‚

**è¿”å›**:
```python
{
    "total_prompts": 2,
    "avg_prompt_length": 150,
    "has_system_prompt_ratio": 0.5,
    "clarity_score": 0.75,
    "detected_issues": [...]
}
```

##### `analyze_performance() -> Dict[str, Any]`

åˆ†ææ€§èƒ½æŒ‡æ ‡ã€‚

##### `analyze_tool_usage() -> Dict[str, Any]`

åˆ†æå·¥å…·ä½¿ç”¨ã€‚

##### `generate_recommendations() -> List[Dict[str, Any]]`

ç”Ÿæˆä¼˜åŒ–å»ºè®®ã€‚

**å»ºè®®æ ¼å¼**:
```python
{
    "category": "token_optimization",
    "severity": "medium",  # high/medium/low
    "title": "Prompt è¿‡é•¿",
    "description": "å¹³å‡ prompt tokens: 2500",
    "suggestion": "è€ƒè™‘ç®€åŒ– prompt..."
}
```

##### `print_report(report=None) -> None`

æ‰“å°æ ¼å¼åŒ–çš„åˆ†ææŠ¥å‘Šã€‚

---

### Python Observer

#### `wrap_python_model(model_class, capture_callback, capture_response=True)`

åŒ…è£… Python æ¨¡å‹ç±»ã€‚

**å‚æ•°**:
- `model_class` (type): è¦åŒ…è£…çš„æ¨¡å‹ç±»
- `capture_callback` (callable): æ•è·å›è°ƒå‡½æ•°
- `capture_response` (bool): æ˜¯å¦æ•è·å“åº”

**è¿”å›**: åŒ…è£…åçš„æ¨¡å‹ç±»

**ç¤ºä¾‹**:
```python
WrappedModel = wrap_python_model(
    GenerativeModel,
    capture_callback=my_callback,
    capture_response=True
)
```

#### `create_observer_callback(capture, capture_id)`

åˆ›å»ºè§‚å¯Ÿå›è°ƒå‡½æ•°ã€‚

**å‚æ•°**:
- `capture`: PromptCapture å®ä¾‹
- `capture_id` (str): æ•è·ä¼šè¯ ID

**è¿”å›**: å›è°ƒå‡½æ•°

**ç¤ºä¾‹**:
```python
callback = create_observer_callback(capture, capture_id)
```

#### `instrument_generative_ai(agent_name, storage_path="./prompt_captures")`

è‡ªåŠ¨ instrument Google Generative AIã€‚

**å‚æ•°**:
- `agent_name` (str): Agent åç§°
- `storage_path` (str): å­˜å‚¨è·¯å¾„

**è¿”å›**: (capture, capture_id, wrapper_function)

**ç¤ºä¾‹**:
```python
capture, capture_id, wrap_model = instrument_generative_ai("my_agent")

WrappedModel = wrap_model(GenerativeModel)
model = WrappedModel("gemini-pro")

# ... ä½¿ç”¨æ¨¡å‹ ...

result = capture.end_capture(capture_id)
```

---

### Node.js Observer

#### `wrapGenerativeModel(ModelClass, options)`

åŒ…è£… Node.js æ¨¡å‹ç±»ã€‚

**å‚æ•°**:
- `ModelClass` (function): æ¨¡å‹ç±»
- `options` (object): é…ç½®é€‰é¡¹
  - `onRequest` (function): è¯·æ±‚å›è°ƒ
  - `onResponse` (function): å“åº”å›è°ƒ
  - `captureEndpoint` (string): è¿œç¨‹ç«¯ç‚¹
  - `autoExport` (boolean): è‡ªåŠ¨å¯¼å‡º
  - `exportPath` (string): å¯¼å‡ºè·¯å¾„

**è¿”å›**: åŒ…è£…åçš„æ¨¡å‹ç±»

**ç¤ºä¾‹**:
```javascript
const WrappedModel = wrapGenerativeModel(GoogleGenerativeAI, {
    onRequest: (data) => {
        console.log('Request:', data.prompt);
    },
    onResponse: (data) => {
        console.log('Response:', data.text);
    },
    autoExport: true,
    exportPath: './captures'
});
```

#### `createShim(outputPath)`

åˆ›å»ºè‡ªåŠ¨æ³¨å…¥ shim æ–‡ä»¶ã€‚

**å‚æ•°**:
- `outputPath` (string): Shim æ–‡ä»¶è·¯å¾„

**ç¤ºä¾‹**:
```javascript
createShim('./tigerhill-shim.js');

// ç„¶åè¿è¡Œï¼š
// NODE_OPTIONS="--require ./tigerhill-shim.js" node app.js
```

---

## ä½¿ç”¨æŒ‡å—

### åœºæ™¯ 1: åŸºç¡€æ•è·

**ç›®æ ‡**: è®°å½• LLM äº¤äº’ç”¨äºè°ƒè¯•

```python
from tigerhill.observer import PromptCapture, wrap_python_model
from tigerhill.observer.python_observer import create_observer_callback
import google.generativeai as genai

# è®¾ç½®
genai.configure(api_key=api_key)
capture = PromptCapture()
capture_id = capture.start_capture("debug_session")
callback = create_observer_callback(capture, capture_id)

# åŒ…è£…
WrappedModel = wrap_python_model(genai.GenerativeModel, callback)
model = WrappedModel("gemini-pro")

# ä½¿ç”¨
response = model.generate_content("Debug this code...")

# ç»“æŸ
result = capture.end_capture(capture_id)
```

### åœºæ™¯ 2: Token ä¼˜åŒ–

**ç›®æ ‡**: åˆ†æå’Œä¼˜åŒ– Token ä½¿ç”¨

```python
# 1. æ•è·æ•°æ®
# ... (åŒåœºæ™¯ 1)

# 2. åˆ†æ
from tigerhill.observer import PromptAnalyzer

analyzer = PromptAnalyzer(result)
token_report = analyzer.analyze_tokens()

# 3. æ£€æŸ¥æ•ˆç‡
if token_report["token_efficiency_ratio"] < 0.5:
    print("âš ï¸ Token efficiency is low")
    print(f"Current: {token_report['token_efficiency_ratio']:.2f}")
    print("Consider requesting more detailed outputs")

# 4. æ£€æŸ¥æˆæœ¬
avg_tokens = token_report["avg_tokens_per_request"]
if avg_tokens > 2000:
    print(f"âš ï¸ High token usage: {avg_tokens:.0f}")
    print("Consider simplifying prompts")
```

### åœºæ™¯ 3: è´¨é‡è¯„ä¼°

**ç›®æ ‡**: è¯„ä¼° prompt è´¨é‡å¹¶æ”¹è¿›

```python
analyzer = PromptAnalyzer(result)
quality_report = analyzer.analyze_prompt_quality()

# æ£€æŸ¥æ¸…æ™°åº¦
if quality_report["clarity_score"] < 0.7:
    print("âš ï¸ Low clarity score")
    issues = quality_report["detected_issues"]
    for issue in issues:
        print(f"  - {issue['type']}: {issue['suggestion']}")

# æ£€æŸ¥ç³»ç»Ÿ prompt
if quality_report["has_system_prompt_ratio"] < 0.8:
    print("ğŸ’¡ Add system prompts for better control")
```

### åœºæ™¯ 4: æ€§èƒ½ç›‘æ§

**ç›®æ ‡**: ç›‘æ§å’Œä¼˜åŒ–å“åº”æ—¶é—´

```python
analyzer = PromptAnalyzer(result)
perf_report = analyzer.analyze_performance()

if perf_report["avg_duration"] > 5.0:
    print(f"âš ï¸ Slow response: {perf_report['avg_duration']:.2f}s")
    print("Consider:")
    print("  - Using faster models")
    print("  - Simplifying prompts")
    print("  - Reducing output length")

# è®°å½•åˆ°ç›‘æ§ç³»ç»Ÿ
metrics.record("llm_response_time", perf_report["avg_duration"])
```

### åœºæ™¯ 5: å·¥å…·ä½¿ç”¨ä¼˜åŒ–

**ç›®æ ‡**: ä¼˜åŒ–å·¥å…·å®šä¹‰å’Œä½¿ç”¨

```python
analyzer = PromptAnalyzer(result)
tool_report = analyzer.analyze_tool_usage()

# æ£€æŸ¥æœªä½¿ç”¨çš„å·¥å…·
unused = tool_report["tools_defined_but_not_used"]
if unused:
    print("âš ï¸ Unused tools detected:")
    for tool in unused:
        print(f"  - {tool}")
    print("Consider removing them to reduce context size")

# æ£€æŸ¥ä½¿ç”¨ç‡
usage_rate = tool_report["tool_usage_rate"]
if usage_rate < 0.1:
    print(f"âš ï¸ Low tool usage: {usage_rate*100:.1f}%")
    print("Tools may not be well-designed or necessary")
```

### åœºæ™¯ 6: æ‰¹é‡åˆ†æ

**ç›®æ ‡**: åˆ†æå¤šä¸ªä¼šè¯çš„è¶‹åŠ¿

```python
# æ”¶é›†å¤šä¸ªæ•è·
captures = []
for capture_file in Path("./captures").glob("capture_*.json"):
    with open(capture_file) as f:
        captures.append(json.load(f))

# æ‰¹é‡åˆ†æ
analyzer = PromptAnalyzer(captures)
report = analyzer.analyze_all()

# è¶‹åŠ¿åˆ†æ
print(f"Total sessions: {report['summary']['total_captures']}")
print(f"Total tokens: {report['token_analysis']['total_tokens']:,}")
print(f"Avg efficiency: {report['token_analysis']['token_efficiency_ratio']:.2f}")
```

### åœºæ™¯ 7: CI/CD é›†æˆ

**ç›®æ ‡**: åœ¨ CI/CD ä¸­è‡ªåŠ¨åˆ†æ

```python
# ci_check_prompts.py
import sys
from pathlib import Path
from tigerhill.observer import PromptAnalyzer

def check_prompts():
    # åŠ è½½æœ€æ–°æ•è·
    captures = load_recent_captures(hours=24)
    analyzer = PromptAnalyzer(captures)
    report = analyzer.analyze_all()

    # æ£€æŸ¥é˜ˆå€¼
    failures = []

    # Token ä½¿ç”¨
    avg_tokens = report["token_analysis"]["avg_tokens_per_request"]
    if avg_tokens > 3000:
        failures.append(f"High token usage: {avg_tokens:.0f} > 3000")

    # è´¨é‡è¯„åˆ†
    clarity = report["prompt_quality"]["clarity_score"]
    if clarity < 0.6:
        failures.append(f"Low clarity score: {clarity:.2f} < 0.6")

    # æ€§èƒ½
    avg_duration = report["performance"]["avg_duration"]
    if avg_duration > 10:
        failures.append(f"Slow response: {avg_duration:.2f}s > 10s")

    # æŠ¥å‘Š
    if failures:
        print("âŒ Prompt quality check failed:")
        for failure in failures:
            print(f"  - {failure}")
        sys.exit(1)
    else:
        print("âœ… Prompt quality check passed")

if __name__ == "__main__":
    check_prompts()
```

**GitHub Actions é…ç½®**:
```yaml
# .github/workflows/prompt-check.yml
name: Prompt Quality Check

on: [push, pull_request]

jobs:
  check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
      - name: Check prompts
        run: python ci_check_prompts.py
```

### åœºæ™¯ 8: ä»æ•è·ç”Ÿæˆæµ‹è¯•

**ç›®æ ‡**: è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹

```python
from tigerhill.observer import PromptCapture
from tigerhill.trace_store import TraceStore

def generate_tests_from_captures(capture_dir="./captures"):
    """ä»æ•è·æ•°æ®ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
    capture = PromptCapture()
    trace_store = TraceStore(storage_path="./tests/traces")

    for capture_file in Path(capture_dir).glob("capture_*.json"):
        # åŠ è½½æ•è·
        with open(capture_file) as f:
            capture_data = json.load(f)

        capture_id = capture_data["capture_id"]
        capture.captures[capture_id] = capture_data

        # å¯¼å‡ºåˆ° TraceStore
        trace_id = capture.export_to_trace_store(
            capture_id,
            trace_store,
            agent_name=capture_data["agent_name"]
        )

        print(f"âœ… Generated test from {capture_file.name}")
        print(f"   Trace ID: {trace_id}")

        # ç”Ÿæˆæµ‹è¯•ä»£ç 
        generate_test_code(trace_id, capture_data)

def generate_test_code(trace_id, capture_data):
    """ç”Ÿæˆæµ‹è¯•ä»£ç """
    test_code = f'''
def test_from_capture_{trace_id[:8]}():
    """Auto-generated from capture"""
    tester = UniversalAgentTester(adapter, trace_store)

'''
    for i, request in enumerate(capture_data["requests"], 1):
        response = capture_data["responses"][i-1] if i <= len(capture_data["responses"]) else None

        test_code += f'''
    # Test case {i}
    result = tester.test({{
        "name": "capture_{trace_id[:8]}_case_{i}",
        "input": {repr(request["prompt"])},
        "expected": {{
            "response_length_min": {len(response["text"]) if response else 0},
            "response_time_max": 10.0
        }}
    }})
    assert result.passed
'''

    # ä¿å­˜æµ‹è¯•æ–‡ä»¶
    test_file = Path(f"tests/test_capture_{trace_id[:8]}.py")
    test_file.write_text(test_code)
    print(f"   Test code: {test_file}")
```

---

## æœ€ä½³å®è·µ

### 1. å¼€å‘ç¯å¢ƒé›†æˆ

#### ç¯å¢ƒå˜é‡æ§åˆ¶

```python
import os

# åªåœ¨å¼€å‘ç¯å¢ƒå¯ç”¨
ENABLE_CAPTURE = os.getenv("TIGERHILL_CAPTURE", "false") == "true"

if ENABLE_CAPTURE:
    from tigerhill.observer import instrument_generative_ai
    capture, capture_id, wrap_model = instrument_generative_ai("my_agent")
    GenerativeModel = wrap_model(GenerativeModel)
```

#### IDE é›†æˆ

åœ¨ VS Code ä¸­æ·»åŠ è°ƒè¯•é…ç½®ï¼š

```json
// .vscode/launch.json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: With Capture",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "env": {
                "TIGERHILL_CAPTURE": "true",
                "TIGERHILL_CAPTURE_PATH": "./debug_captures"
            }
        }
    ]
}
```

### 2. æ•è·ç­–ç•¥

#### é‡‡æ ·æ•è·

ä¸æ˜¯æ‰€æœ‰è¯·æ±‚éƒ½éœ€è¦æ•è·ï¼Œå¯ä»¥é‡‡æ ·ï¼š

```python
import random

class SamplingCapture:
    def __init__(self, capture, sample_rate=0.1):
        self.capture = capture
        self.sample_rate = sample_rate
        self.capture_id = None

    def should_capture(self):
        return random.random() < self.sample_rate

    def start_if_needed(self, agent_name):
        if self.should_capture():
            self.capture_id = self.capture.start_capture(agent_name)
        return self.capture_id

# ä½¿ç”¨
sampling = SamplingCapture(capture, sample_rate=0.1)  # 10% é‡‡æ ·
```

#### æ¡ä»¶æ•è·

åªåœ¨ç‰¹å®šæ¡ä»¶ä¸‹æ•è·ï¼š

```python
def conditional_capture(request_data):
    # åªæ•è·é•¿ prompt
    if len(request_data.get("prompt", "")) > 1000:
        return True

    # åªæ•è·ä½¿ç”¨å·¥å…·çš„è¯·æ±‚
    if request_data.get("tools"):
        return True

    return False
```

### 3. æ€§èƒ½ä¼˜åŒ–

#### å¼‚æ­¥æ•è·

```python
import asyncio
from queue import Queue
from threading import Thread

class AsyncCapture:
    def __init__(self, capture):
        self.capture = capture
        self.queue = Queue()
        self.worker = Thread(target=self._process_queue, daemon=True)
        self.worker.start()

    def capture_async(self, capture_id, data, is_response=False):
        self.queue.put((capture_id, data, is_response))

    def _process_queue(self):
        while True:
            capture_id, data, is_response = self.queue.get()
            if is_response:
                self.capture.capture_response(capture_id, data)
            else:
                self.capture.capture_request(capture_id, data)
            self.queue.task_done()
```

#### æ‰¹é‡ä¿å­˜

```python
class BatchCapture:
    def __init__(self, capture, batch_size=10):
        self.capture = capture
        self.batch_size = batch_size
        self.buffer = []

    def capture_request(self, capture_id, data):
        self.buffer.append(("request", capture_id, data))
        if len(self.buffer) >= self.batch_size:
            self.flush()

    def flush(self):
        for type, capture_id, data in self.buffer:
            if type == "request":
                self.capture.capture_request(capture_id, data)
            else:
                self.capture.capture_response(capture_id, data)
        self.buffer.clear()
```

### 4. æ•°æ®ç®¡ç†

#### è‡ªåŠ¨æ¸…ç†

```python
from datetime import datetime, timedelta
from pathlib import Path

def cleanup_old_captures(capture_dir="./captures", days=7):
    """åˆ é™¤è¶…è¿‡ N å¤©çš„æ•è·æ•°æ®"""
    cutoff = datetime.now() - timedelta(days=days)

    for capture_file in Path(capture_dir).glob("capture_*.json"):
        mtime = datetime.fromtimestamp(capture_file.stat().st_mtime)
        if mtime < cutoff:
            capture_file.unlink()
            print(f"Deleted old capture: {capture_file.name}")
```

#### å‹ç¼©å­˜å‚¨

```python
import gzip
import json

def compress_capture(capture_file):
    """å‹ç¼©æ•è·æ–‡ä»¶"""
    with open(capture_file) as f:
        data = json.load(f)

    compressed_file = capture_file.with_suffix(".json.gz")
    with gzip.open(compressed_file, "wt", encoding="utf-8") as f:
        json.dump(data, f)

    capture_file.unlink()  # åˆ é™¤åŸæ–‡ä»¶
    return compressed_file
```

### 5. éšç§å’Œå®‰å…¨

#### è„±æ•éªŒè¯

```python
def verify_redaction(capture_data):
    """éªŒè¯è„±æ•æ˜¯å¦å®Œæ•´"""
    sensitive_patterns = [
        r"AIza[0-9A-Za-z-_]{20,}",
        r"sk-[0-9A-Za-z-_]{20,}",
        r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}"
    ]

    import re

    def check_text(text):
        for pattern in sensitive_patterns:
            if re.search(pattern, text):
                return False
        return True

    # æ£€æŸ¥æ‰€æœ‰æ–‡æœ¬å­—æ®µ
    for request in capture_data.get("requests", []):
        if not check_text(str(request)):
            raise ValueError("Sensitive data detected in requests!")

    for response in capture_data.get("responses", []):
        if not check_text(str(response)):
            raise ValueError("Sensitive data detected in responses!")

    return True
```

#### åŠ å¯†å­˜å‚¨

```python
from cryptography.fernet import Fernet

class EncryptedCapture:
    def __init__(self, capture, key=None):
        self.capture = capture
        self.key = key or Fernet.generate_key()
        self.cipher = Fernet(self.key)

    def save_encrypted(self, capture_data, filepath):
        # åºåˆ—åŒ–
        json_data = json.dumps(capture_data)

        # åŠ å¯†
        encrypted = self.cipher.encrypt(json_data.encode())

        # ä¿å­˜
        with open(filepath, "wb") as f:
            f.write(encrypted)

    def load_encrypted(self, filepath):
        # è¯»å–
        with open(filepath, "rb") as f:
            encrypted = f.read()

        # è§£å¯†
        decrypted = self.cipher.decrypt(encrypted)

        # ååºåˆ—åŒ–
        return json.loads(decrypted.decode())
```

### 6. ç›‘æ§å’Œå‘Šè­¦

#### Prometheus é›†æˆ

```python
from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
capture_requests_total = Counter("tigerhill_capture_requests_total", "Total capture requests")
capture_tokens_total = Counter("tigerhill_capture_tokens_total", "Total tokens captured")
capture_duration_seconds = Histogram("tigerhill_capture_duration_seconds", "Capture duration")
active_captures = Gauge("tigerhill_active_captures", "Active capture sessions")

class MonitoredCapture:
    def __init__(self, capture):
        self.capture = capture

    def start_capture(self, agent_name):
        capture_id = self.capture.start_capture(agent_name)
        active_captures.inc()
        return capture_id

    def capture_request(self, capture_id, data):
        self.capture.capture_request(capture_id, data)
        capture_requests_total.inc()

    def capture_response(self, capture_id, data):
        self.capture.capture_response(capture_id, data)
        if data.get("usage"):
            tokens = data["usage"].get("total_tokens", 0)
            capture_tokens_total.inc(tokens)

    def end_capture(self, capture_id):
        result = self.capture.end_capture(capture_id)
        active_captures.dec()
        capture_duration_seconds.observe(result["duration"])
        return result
```

#### Slack å‘Šè­¦

```python
import requests

def send_slack_alert(webhook_url, message):
    """å‘é€ Slack å‘Šè­¦"""
    payload = {"text": message}
    requests.post(webhook_url, json=payload)

def check_and_alert(report, webhook_url):
    """æ£€æŸ¥æŠ¥å‘Šå¹¶å‘é€å‘Šè­¦"""
    alerts = []

    # Token ä½¿ç”¨è¿‡é«˜
    avg_tokens = report["token_analysis"]["avg_tokens_per_request"]
    if avg_tokens > 3000:
        alerts.append(f"âš ï¸ High token usage: {avg_tokens:.0f} tokens/request")

    # å“åº”æ—¶é—´è¿‡é•¿
    avg_duration = report["performance"]["avg_duration"]
    if avg_duration > 10:
        alerts.append(f"âš ï¸ Slow response: {avg_duration:.2f}s")

    # è´¨é‡è¯„åˆ†ä½
    clarity = report["prompt_quality"]["clarity_score"]
    if clarity < 0.5:
        alerts.append(f"âš ï¸ Low prompt quality: {clarity:.2f}/1.0")

    if alerts:
        message = "TigerHill Observer Alerts:\n" + "\n".join(alerts)
        send_slack_alert(webhook_url, message)
```

---

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ•è·æ•°æ®ä¸ºç©º

**ç—‡çŠ¶**: `end_capture()` è¿”å›ç©ºçš„ requests/responses åˆ—è¡¨

**åŸå› **:
- åŒ…è£…å™¨æœªæ­£ç¡®åº”ç”¨
- å›è°ƒå‡½æ•°æœªæ­£ç¡®è®¾ç½®
- Capture ID ä¸åŒ¹é…

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥åŒ…è£…æ˜¯å¦æˆåŠŸ
print(type(model))  # åº”è¯¥æ˜¯ WrappedGenerativeModel

# æ£€æŸ¥ callback æ˜¯å¦è¢«è°ƒç”¨
def debug_callback(data):
    print(f"Callback called: {data}")
    capture.capture_request(capture_id, data)

# ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ capture_id
print(f"Using capture_id: {capture_id}")
```

#### 2. JSON åºåˆ—åŒ–é”™è¯¯

**ç—‡çŠ¶**: `TypeError: Object of type X is not JSON serializable`

**åŸå› **:
- æ•è·çš„æ•°æ®åŒ…å«ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡ï¼ˆå¦‚ Mock å¯¹è±¡ï¼‰

**è§£å†³æ–¹æ¡ˆ**:
```python
# ç¦ç”¨ auto_save è¿›è¡Œæµ‹è¯•
capture = PromptCapture(auto_save=False)

# æˆ–è‡ªå®šä¹‰åºåˆ—åŒ–
class CustomEncoder(json.JSONEncoder):
    def default(self, obj):
        if hasattr(obj, "__dict__"):
            return obj.__dict__
        return str(obj)

json.dump(data, f, cls=CustomEncoder)
```

#### 3. è„±æ•ä¸å®Œæ•´

**ç—‡çŠ¶**: æ•è·æ•°æ®ä¸­ä»åŒ…å«æ•æ„Ÿä¿¡æ¯

**åŸå› **:
- é»˜è®¤è§„åˆ™æœªè¦†ç›–
- è‡ªå®šä¹‰æ ¼å¼çš„æ•æ„Ÿæ•°æ®

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ·»åŠ è‡ªå®šä¹‰è§„åˆ™
custom_patterns = [
    {"pattern": r"your-custom-pattern", "replacement": "<REDACTED>"}
]

capture = PromptCapture(redact_patterns=custom_patterns)

# éªŒè¯
verify_redaction(capture_data)
```

#### 4. æ€§èƒ½å½±å“

**ç—‡çŠ¶**: åº”ç”¨å˜æ…¢

**åŸå› **:
- åŒæ­¥æ•è·é˜»å¡ä¸»æµç¨‹
- é¢‘ç¹çš„æ–‡ä»¶ I/O

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨å¼‚æ­¥æ•è·
async_capture = AsyncCapture(capture)

# ç¦ç”¨ auto_save
capture = PromptCapture(auto_save=False)

# æ‰¹é‡ä¿å­˜
batch_capture = BatchCapture(capture, batch_size=20)
```

#### 5. å†…å­˜å ç”¨è¿‡é«˜

**ç—‡çŠ¶**: å†…å­˜æŒç»­å¢é•¿

**åŸå› **:
- é•¿æ—¶é—´è¿è¡Œæœªé‡Šæ”¾æ•è·æ•°æ®
- å¤§é‡æ–‡æœ¬æ•°æ®å †ç§¯

**è§£å†³æ–¹æ¡ˆ**:
```python
# å®šæœŸç»“æŸæ•è·
if time.time() - start_time > 3600:  # 1 å°æ—¶
    capture.end_capture(capture_id)
    capture_id = capture.start_capture(agent_name)

# é™åˆ¶å†å²æ•°æ®
max_captures = 100
if len(capture.captures) > max_captures:
    # ç§»é™¤æœ€æ—§çš„
    oldest = min(capture.captures.keys())
    del capture.captures[oldest]
```

### è°ƒè¯•æŠ€å·§

#### å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger("tigerhill.observer")
logger.setLevel(logging.DEBUG)
```

#### éªŒè¯æ•è·æµç¨‹

```python
def verify_capture_flow():
    """éªŒè¯æ•è·æµç¨‹æ˜¯å¦æ­£å¸¸"""
    capture = PromptCapture()

    # 1. å¼€å§‹æ•è·
    capture_id = capture.start_capture("test")
    assert capture_id in capture.captures
    print("âœ… Start capture OK")

    # 2. æ•è·è¯·æ±‚
    capture.capture_request(capture_id, {"model": "test", "prompt": "hi"})
    assert len(capture.get_capture(capture_id)["requests"]) == 1
    print("âœ… Capture request OK")

    # 3. æ•è·å“åº”
    capture.capture_response(capture_id, {"text": "hello"})
    assert len(capture.get_capture(capture_id)["responses"]) == 1
    print("âœ… Capture response OK")

    # 4. ç»“æŸæ•è·
    result = capture.end_capture(capture_id)
    assert result["status"] == "completed"
    print("âœ… End capture OK")

    print("\nâœ… All verification passed!")

verify_capture_flow()
```

#### æ£€æŸ¥åŒ…è£…å™¨

```python
def check_wrapper():
    """æ£€æŸ¥åŒ…è£…å™¨æ˜¯å¦æ­£ç¡®åº”ç”¨"""
    from tigerhill.observer import wrap_python_model

    # åˆ›å»º mock ç±»
    class MockModel:
        def generate_content(self, prompt):
            return f"Response to: {prompt}"

    # åŒ…è£…
    captured = []
    WrappedModel = wrap_python_model(
        MockModel,
        capture_callback=lambda data: captured.append(data)
    )

    # æµ‹è¯•
    model = WrappedModel()
    result = model.generate_content("test")

    # éªŒè¯
    assert len(captured) > 0, "No data captured!"
    assert captured[0].get("prompt") == "test"
    print("âœ… Wrapper working correctly")

check_wrapper()
```

### è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. **æŸ¥çœ‹æ—¥å¿—**: å¯ç”¨ DEBUG çº§åˆ«æ—¥å¿—
2. **è¿è¡ŒéªŒè¯**: ä½¿ç”¨ä¸Šè¿°éªŒè¯è„šæœ¬
3. **æ£€æŸ¥ç‰ˆæœ¬**: ç¡®ä¿ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
4. **æŸ¥çœ‹ç¤ºä¾‹**: å‚è€ƒ `examples/` ç›®å½•
5. **æäº¤ Issue**: GitHub Issues

---

## æ€»ç»“

TigerHill Observer SDK æä¾›äº†ï¼š

âœ… **æ— ä¾µå…¥å¼æ•è·** - é€æ˜åŒ…è£…ï¼Œä¸æ”¹ä»£ç 
âœ… **è‡ªåŠ¨åˆ†æ** - 5 ç»´åº¦æ·±åº¦åˆ†æ
âœ… **éšç§ä¿æŠ¤** - è‡ªåŠ¨è„±æ•æ•æ„Ÿä¿¡æ¯
âœ… **æ— ç¼é›†æˆ** - å¯¼å‡ºåˆ° TraceStore
âœ… **è·¨è¯­è¨€æ”¯æŒ** - Python å’Œ Node.js

**å¼€å§‹ä½¿ç”¨**:
```bash
# Python
python examples/observer_python_basic.py

# Node.js
node examples/observer_nodejs_basic.js
```

**ä¸‹ä¸€æ­¥**:
- åœ¨å¼€å‘ç¯å¢ƒé›†æˆ Observer
- å®šæœŸè¿è¡Œåˆ†æè·å–ä¼˜åŒ–å»ºè®®
- å°†æ•è·æ•°æ®è½¬æ¢ä¸ºæµ‹è¯•ç”¨ä¾‹
- é›†æˆåˆ° CI/CD æµç¨‹

---

ğŸ“š **ç›¸å…³æ–‡æ¡£**:
- [å¿«é€Ÿå¼€å§‹](./QUICK_START.md)
- [API å‚è€ƒ](./API_REFERENCE.md)
- [ç¤ºä¾‹ä»£ç ](./examples/README.md)
- [æœ€ä½³å®è·µ](./BEST_PRACTICES.md)

ğŸ¤ **ç¤¾åŒº**:
- [GitHub Issues](https://github.com/yourusername/tigerhill/issues)
- [Discussions](https://github.com/yourusername/tigerhill/discussions)

ğŸ“ **License**: MIT
