"""
Phase 1 é›†æˆæµ‹è¯•

æ¨¡æ‹ŸçœŸå®çš„gemini-cliå¤šè½®å¯¹è¯åœºæ™¯ï¼Œæµ‹è¯•å®Œæ•´çš„promptæ•è·æµç¨‹
"""

import pytest
import json
import time
from pathlib import Path
from tigerhill.observer import PromptCapture
from tigerhill.observer.conversation_models import MessageRole


class TestGeminiCLIIntegration:
    """æ¨¡æ‹Ÿgemini-cliçš„é›†æˆæµ‹è¯•"""

    def test_complete_multiturn_conversation_flow(self, tmp_path):
        """
        æµ‹è¯•å®Œæ•´çš„å¤šè½®å¯¹è¯æµç¨‹

        æ¨¡æ‹Ÿåœºæ™¯ï¼šç”¨æˆ·ä½¿ç”¨gemini-cliè¿›è¡Œä»£ç é‡æ„å’¨è¯¢
        """
        print("\n" + "=" * 80)
        print("ğŸ§ª æµ‹è¯•åœºæ™¯ï¼šGemini CLI ä»£ç é‡æ„å’¨è¯¢ï¼ˆ3è½®å¯¹è¯ï¼‰")
        print("=" * 80)

        # åˆå§‹åŒ–æ•è·å™¨
        capture = PromptCapture(storage_path=str(tmp_path / "captures"))
        capture_id = capture.start_capture("gemini-cli")
        conversation_id = "conv_refactoring_001"

        # å®šä¹‰ç³»ç»Ÿpromptï¼ˆé€šå¸¸åœ¨gemini-cliå¯åŠ¨æ—¶è®¾ç½®ï¼‰
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonä»£ç å®¡æŸ¥å’Œé‡æ„åŠ©æ‰‹ã€‚
ä½ çš„èŒè´£æ˜¯ï¼š
1. åˆ†æä»£ç è´¨é‡å’Œæ½œåœ¨é—®é¢˜
2. æä¾›å…·ä½“çš„é‡æ„å»ºè®®
3. è§£é‡Šé‡æ„çš„ç†ç”±å’Œå¥½å¤„"""

        print(f"\nğŸ“ ç³»ç»ŸPromptå·²è®¾ç½®:")
        print(f"   {system_prompt[:60]}...")

        # ===== Turn 1: ç”¨æˆ·è¯·æ±‚ä»£ç å®¡æŸ¥ =====
        print("\n" + "-" * 80)
        print("ğŸ”µ Turn 1: ç”¨æˆ·è¯·æ±‚ä»£ç å®¡æŸ¥")
        print("-" * 80)

        user_code = """
def calc(a, b, op):
    if op == 'add':
        return a + b
    elif op == 'sub':
        return a - b
    elif op == 'mul':
        return a * b
    elif op == 'div':
        return a / b
"""

        turn1_request = {
            "request_id": "req_001",
            "model": "gemini-2.0-flash-exp",
            "system_prompt": system_prompt,
            "prompt": f"è¯·å®¡æŸ¥è¿™æ®µä»£ç å¹¶æä¾›é‡æ„å»ºè®®ï¼š\n```python{user_code}```",
            "timestamp": time.time(),
            "generation_config": {
                "temperature": 0.7,
                "max_tokens": 2000
            }
        }

        req_id_1 = capture.capture_request(
            capture_id,
            turn1_request,
            conversation_id=conversation_id,
            turn_number=1
        )

        print(f"   User: è¯·å®¡æŸ¥è¿™æ®µä»£ç å¹¶æä¾›é‡æ„å»ºè®®...")
        print(f"   ğŸ“¤ Request ID: {req_id_1}")

        # æ¨¡æ‹ŸLLMå“åº”
        turn1_response = {
            "request_id": req_id_1,
            "text": """ä»£ç å®¡æŸ¥ç»“æœï¼š

1. **é—®é¢˜åˆ†æ**ï¼š
   - ä½¿ç”¨å¤§é‡if-elifå¯¼è‡´ä»£ç ä¸æ˜“æ‰©å±•
   - ç¼ºå°‘é”™è¯¯å¤„ç†ï¼ˆé™¤é›¶é”™è¯¯ï¼‰
   - å‡½æ•°å‘½åä¸å¤Ÿæè¿°æ€§

2. **é‡æ„å»ºè®®**ï¼š
   - ä½¿ç”¨å­—å…¸æ˜ å°„æ›¿ä»£if-elif
   - æ·»åŠ è¾“å…¥éªŒè¯
   - æ”¹è¿›å‡½æ•°å‘½åå’Œæ–‡æ¡£

å»ºè®®ä½¿ç”¨ç­–ç•¥æ¨¡å¼é‡æ„ã€‚""",
            "finish_reason": "STOP",
            "usage": {
                "prompt_tokens": 150,
                "completion_tokens": 120,
                "total_tokens": 270
            },
            "duration": 2.3,
            "timestamp": time.time()
        }

        capture.capture_response(capture_id, turn1_response, request_id=req_id_1)

        print(f"   Assistant: ä»£ç å®¡æŸ¥ç»“æœï¼šé—®é¢˜åˆ†æå’Œé‡æ„å»ºè®®...")
        print(f"   ğŸ“Š Tokens: {turn1_response['usage']['total_tokens']}")
        print(f"   â±ï¸  Duration: {turn1_response['duration']:.2f}s")

        # ===== Turn 2: ç”¨æˆ·è¯·æ±‚é‡æ„ç¤ºä¾‹ =====
        print("\n" + "-" * 80)
        print("ğŸ”µ Turn 2: ç”¨æˆ·è¯·æ±‚é‡æ„ç¤ºä¾‹")
        print("-" * 80)

        turn2_request = {
            "request_id": "req_002",
            "model": "gemini-2.0-flash-exp",
            "prompt": "è¯·ç»™å‡ºä½¿ç”¨ç­–ç•¥æ¨¡å¼é‡æ„åçš„ä»£ç ç¤ºä¾‹",
            "timestamp": time.time(),
            "generation_config": {
                "temperature": 0.7,
                "max_tokens": 2000
            }
        }

        req_id_2 = capture.capture_request(
            capture_id,
            turn2_request,
            conversation_id=conversation_id,
            turn_number=2
        )

        print(f"   User: è¯·ç»™å‡ºä½¿ç”¨ç­–ç•¥æ¨¡å¼é‡æ„åçš„ä»£ç ç¤ºä¾‹")
        print(f"   ğŸ“¤ Request ID: {req_id_2}")

        refactored_code = """
class Calculator:
    def __init__(self):
        self.operations = {
            'add': lambda a, b: a + b,
            'sub': lambda a, b: a - b,
            'mul': lambda a, b: a * b,
            'div': lambda a, b: a / b if b != 0 else None
        }

    def calculate(self, a: float, b: float, operation: str) -> float:
        if operation not in self.operations:
            raise ValueError(f"Unsupported operation: {operation}")

        result = self.operations[operation](a, b)
        if result is None:
            raise ValueError("Division by zero")

        return result
"""

        turn2_response = {
            "request_id": req_id_2,
            "text": f"è¿™æ˜¯é‡æ„åçš„ä»£ç ï¼š\n```python{refactored_code}```\n\næ”¹è¿›ç‚¹ï¼š\n1. ä½¿ç”¨å­—å…¸æ˜ å°„æ¶ˆé™¤if-elif\n2. æ·»åŠ äº†ç±»å‹æç¤º\n3. æ·»åŠ äº†è¾“å…¥éªŒè¯å’Œé”™è¯¯å¤„ç†\n4. ä»£ç æ›´æ˜“æ‰©å±•ï¼ˆæ·»åŠ æ–°æ“ä½œåªéœ€æ·»åŠ å­—å…¸é¡¹ï¼‰",
            "finish_reason": "STOP",
            "usage": {
                "prompt_tokens": 180,
                "completion_tokens": 200,
                "total_tokens": 380
            },
            "duration": 3.1,
            "timestamp": time.time()
        }

        capture.capture_response(capture_id, turn2_response, request_id=req_id_2)

        print(f"   Assistant: è¿™æ˜¯é‡æ„åçš„ä»£ç ï¼š[ä»£ç ç¤ºä¾‹]...")
        print(f"   ğŸ“Š Tokens: {turn2_response['usage']['total_tokens']}")
        print(f"   â±ï¸  Duration: {turn2_response['duration']:.2f}s")

        # ===== Turn 3: ç”¨æˆ·è¯¢é—®æµ‹è¯•å»ºè®® =====
        print("\n" + "-" * 80)
        print("ğŸ”µ Turn 3: ç”¨æˆ·è¯¢é—®æµ‹è¯•å»ºè®®")
        print("-" * 80)

        turn3_request = {
            "request_id": "req_003",
            "model": "gemini-2.0-flash-exp",
            "prompt": "å¦‚ä½•ä¸ºè¿™ä¸ªé‡æ„åçš„ä»£ç ç¼–å†™å•å…ƒæµ‹è¯•ï¼Ÿ",
            "timestamp": time.time()
        }

        req_id_3 = capture.capture_request(
            capture_id,
            turn3_request,
            conversation_id=conversation_id,
            turn_number=3
        )

        print(f"   User: å¦‚ä½•ä¸ºè¿™ä¸ªé‡æ„åçš„ä»£ç ç¼–å†™å•å…ƒæµ‹è¯•ï¼Ÿ")
        print(f"   ğŸ“¤ Request ID: {req_id_3}")

        turn3_response = {
            "request_id": req_id_3,
            "text": """å•å…ƒæµ‹è¯•å»ºè®®ï¼š

```python
import pytest

def test_calculator_add():
    calc = Calculator()
    assert calc.calculate(2, 3, 'add') == 5

def test_calculator_division_by_zero():
    calc = Calculator()
    with pytest.raises(ValueError):
        calc.calculate(5, 0, 'div')

def test_invalid_operation():
    calc = Calculator()
    with pytest.raises(ValueError):
        calc.calculate(1, 2, 'invalid')
```

æµ‹è¯•è¦†ç›–äº†ï¼šæ­£å¸¸æ“ä½œã€è¾¹ç•Œæ¡ä»¶ã€å¼‚å¸¸å¤„ç†ã€‚""",
            "finish_reason": "STOP",
            "usage": {
                "prompt_tokens": 200,
                "completion_tokens": 150,
                "total_tokens": 350
            },
            "duration": 2.8,
            "timestamp": time.time()
        }

        capture.capture_response(capture_id, turn3_response, request_id=req_id_3)

        print(f"   Assistant: å•å…ƒæµ‹è¯•å»ºè®®ï¼š[æµ‹è¯•ä»£ç ç¤ºä¾‹]...")
        print(f"   ğŸ“Š Tokens: {turn3_response['usage']['total_tokens']}")
        print(f"   â±ï¸  Duration: {turn3_response['duration']:.2f}s")

        # ===== éªŒè¯æ•è·ç»“æœ =====
        print("\n" + "=" * 80)
        print("âœ… éªŒè¯æ•è·ç»“æœ")
        print("=" * 80)

        # 1. éªŒè¯å¯¹è¯å†å²
        conv = capture.get_conversation_history(conversation_id)

        assert conv is not None, "å¯¹è¯å†å²åº”è¯¥å­˜åœ¨"
        assert conv.conversation_id == conversation_id
        assert conv.agent_name == "gemini-cli"

        print(f"\nâœ“ å¯¹è¯å†å²éªŒè¯:")
        print(f"  â€¢ å¯¹è¯ID: {conv.conversation_id}")
        print(f"  â€¢ Agent: {conv.agent_name}")
        print(f"  â€¢ æ€»è½®æ¬¡: {conv.total_turns}")
        assert conv.total_turns == 3, "åº”è¯¥æœ‰3è½®å¯¹è¯"

        # 2. éªŒè¯ç³»ç»Ÿprompt
        assert conv.system_prompt is not None, "åº”è¯¥æ•è·ç³»ç»Ÿprompt"
        assert "ä»£ç å®¡æŸ¥å’Œé‡æ„åŠ©æ‰‹" in conv.system_prompt
        print(f"  â€¢ ç³»ç»ŸPrompt: âœ“ å·²æ•è· ({len(conv.system_prompt)} å­—ç¬¦)")

        # 3. éªŒè¯æ¶ˆæ¯ç»“æ„
        print(f"\nâœ“ æ¶ˆæ¯ç»“æ„éªŒè¯:")
        print(f"  â€¢ æ€»æ¶ˆæ¯æ•°: {len(conv.messages)}")
        assert len(conv.messages) == 7, "åº”è¯¥æœ‰7æ¡æ¶ˆæ¯ï¼ˆ1 system + 3*2 user/assistantï¼‰"

        # éªŒè¯æ¶ˆæ¯è§’è‰²
        roles = [msg.role for msg in conv.messages]
        expected_roles = [
            MessageRole.SYSTEM,
            MessageRole.USER, MessageRole.ASSISTANT,
            MessageRole.USER, MessageRole.ASSISTANT,
            MessageRole.USER, MessageRole.ASSISTANT
        ]
        assert roles == expected_roles, "æ¶ˆæ¯è§’è‰²é¡ºåºåº”è¯¥æ­£ç¡®"
        print(f"  â€¢ è§’è‰²åºåˆ—: system â†’ user â†’ assistant â†’ user â†’ assistant â†’ user â†’ assistant âœ“")

        # 4. éªŒè¯turnç¼–å·
        turn_numbers = [msg.turn_number for msg in conv.messages if msg.role != MessageRole.SYSTEM]
        expected_turns = [1, 1, 2, 2, 3, 3]
        assert turn_numbers == expected_turns, "Turnç¼–å·åº”è¯¥æ­£ç¡®"
        print(f"  â€¢ Turnç¼–å·: {turn_numbers} âœ“")

        # 5. éªŒè¯tokensç»Ÿè®¡
        total_tokens = conv.total_tokens["total_tokens"]
        expected_total = 270 + 380 + 350  # 1000
        assert total_tokens == expected_total, f"Tokenæ€»æ•°åº”è¯¥ä¸º{expected_total}"
        print(f"\nâœ“ Tokenç»Ÿè®¡éªŒè¯:")
        print(f"  â€¢ Prompt tokens: {conv.total_tokens['prompt_tokens']}")
        print(f"  â€¢ Completion tokens: {conv.total_tokens['completion_tokens']}")
        print(f"  â€¢ Total tokens: {total_tokens} âœ“")

        # 6. éªŒè¯å¯¹è¯æ‘˜è¦
        summary = capture.get_conversation_summary(conversation_id)
        assert summary["total_turns"] == 3
        assert summary["total_messages"] == 7
        assert summary["has_system_prompt"] == True
        print(f"\nâœ“ å¯¹è¯æ‘˜è¦éªŒè¯:")
        print(f"  â€¢ æ€»è½®æ¬¡: {summary['total_turns']} âœ“")
        print(f"  â€¢ æ€»æ¶ˆæ¯: {summary['total_messages']} âœ“")
        print(f"  â€¢ æœ‰ç³»ç»ŸPrompt: {summary['has_system_prompt']} âœ“")
        print(f"  â€¢ å¯¹è¯æ—¶é•¿: {summary['duration']:.3f}s")

        # 7. éªŒè¯æ¶ˆæ¯å†…å®¹å®Œæ•´æ€§
        print(f"\nâœ“ æ¶ˆæ¯å†…å®¹éªŒè¯:")
        for msg in conv.messages:
            assert len(msg.content) > 0, "æ¶ˆæ¯å†…å®¹ä¸åº”ä¸ºç©º"
            assert msg.timestamp > 0, "æ—¶é—´æˆ³åº”è¯¥æœ‰æ•ˆ"
        print(f"  â€¢ æ‰€æœ‰æ¶ˆæ¯å†…å®¹å®Œæ•´ âœ“")
        print(f"  â€¢ æ‰€æœ‰æ—¶é—´æˆ³æœ‰æ•ˆ âœ“")

        # 8. å¯¼å‡ºå¯¹è¯å†å²
        export_path = tmp_path / "conversation_export.json"
        capture.export_conversation(conversation_id, str(export_path))

        assert export_path.exists(), "å¯¼å‡ºæ–‡ä»¶åº”è¯¥å­˜åœ¨"

        with open(export_path, 'r', encoding='utf-8') as f:
            exported_data = json.load(f)

        assert exported_data["conversation_id"] == conversation_id
        assert exported_data["total_turns"] == 3
        assert len(exported_data["messages"]) == 7
        print(f"\nâœ“ å¯¼å‡ºéªŒè¯:")
        print(f"  â€¢ å¯¼å‡ºæ–‡ä»¶: {export_path.name} âœ“")
        print(f"  â€¢ æ–‡ä»¶å¤§å°: {export_path.stat().st_size} bytes")

        # 9. ç»“æŸæ•è·
        result = capture.end_capture(capture_id)
        print(f"\nâœ“ æ•è·ä¼šè¯å®Œæˆ:")
        print(f"  â€¢ æ€»è¯·æ±‚æ•°: {result['statistics']['total_requests']}")
        print(f"  â€¢ æ€»å“åº”æ•°: {result['statistics']['total_responses']}")
        print(f"  â€¢ ä¼šè¯æ—¶é•¿: {result['duration']:.3f}s")

        print("\n" + "=" * 80)
        print("ğŸ‰ é›†æˆæµ‹è¯•å®Œæˆï¼æ‰€æœ‰éªŒè¯é€šè¿‡ï¼")
        print("=" * 80)

    def test_multiple_conversations_in_same_session(self, tmp_path):
        """æµ‹è¯•åœ¨åŒä¸€ä¸ªcapture sessionä¸­å¤„ç†å¤šä¸ªå¯¹è¯"""
        print("\n" + "=" * 80)
        print("ğŸ§ª æµ‹è¯•åœºæ™¯ï¼šåŒä¸€ä¼šè¯ä¸­çš„å¤šä¸ªç‹¬ç«‹å¯¹è¯")
        print("=" * 80)

        capture = PromptCapture(storage_path=str(tmp_path / "multi_conv"))
        capture_id = capture.start_capture("gemini-cli-multi")

        # å¯¹è¯1ï¼šå…³äºPython
        conv1_id = "conv_python_001"
        print("\nğŸ“ å¯¹è¯1: PythonåŸºç¡€")
        for turn in range(1, 3):
            req_id = capture.capture_request(
                capture_id,
                {
                    "model": "gemini-2.0",
                    "prompt": f"Pythoné—®é¢˜ {turn}",
                    "system_prompt": "PythonåŠ©æ‰‹" if turn == 1 else None
                },
                conversation_id=conv1_id,
                turn_number=turn
            )
            capture.capture_response(
                capture_id,
                {"text": f"Pythonå›ç­” {turn}", "usage": {"total_tokens": 50}},
                request_id=req_id
            )
            print(f"   Turn {turn}: å®Œæˆ")

        # å¯¹è¯2ï¼šå…³äºJavaScript
        conv2_id = "conv_javascript_001"
        print("\nğŸ“ å¯¹è¯2: JavaScriptåŸºç¡€")
        for turn in range(1, 3):
            req_id = capture.capture_request(
                capture_id,
                {
                    "model": "gemini-2.0",
                    "prompt": f"JavaScripté—®é¢˜ {turn}",
                    "system_prompt": "JavaScriptåŠ©æ‰‹" if turn == 1 else None
                },
                conversation_id=conv2_id,
                turn_number=turn
            )
            capture.capture_response(
                capture_id,
                {"text": f"JavaScriptå›ç­” {turn}", "usage": {"total_tokens": 50}},
                request_id=req_id
            )
            print(f"   Turn {turn}: å®Œæˆ")

        # éªŒè¯ä¸¤ä¸ªå¯¹è¯éƒ½è¢«æ­£ç¡®è¿½è¸ª
        conversations = capture.list_conversations()
        assert len(conversations) == 2, "åº”è¯¥æœ‰2ä¸ªå¯¹è¯"
        print(f"\nâœ“ å¯¹è¯åˆ—è¡¨éªŒè¯:")
        print(f"  â€¢ å¯¹è¯æ€»æ•°: {len(conversations)} âœ“")

        conv1 = capture.get_conversation_history(conv1_id)
        conv2 = capture.get_conversation_history(conv2_id)

        assert conv1.total_turns == 2
        assert conv2.total_turns == 2
        assert conv1.system_prompt == "PythonåŠ©æ‰‹"
        assert conv2.system_prompt == "JavaScriptåŠ©æ‰‹"

        print(f"  â€¢ å¯¹è¯1 ({conv1_id}): {conv1.total_turns}è½®, ç³»ç»Ÿprompt='{conv1.system_prompt}' âœ“")
        print(f"  â€¢ å¯¹è¯2 ({conv2_id}): {conv2.total_turns}è½®, ç³»ç»Ÿprompt='{conv2.system_prompt}' âœ“")

        print("\nğŸ‰ å¤šå¯¹è¯æµ‹è¯•å®Œæˆï¼")

    def test_conversation_without_system_prompt(self, tmp_path):
        """æµ‹è¯•æ²¡æœ‰ç³»ç»Ÿpromptçš„å¯¹è¯ï¼ˆå…¼å®¹æ€§æµ‹è¯•ï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸ§ª æµ‹è¯•åœºæ™¯ï¼šæ— ç³»ç»ŸPromptçš„å¯¹è¯ï¼ˆå‘åå…¼å®¹ï¼‰")
        print("=" * 80)

        capture = PromptCapture(storage_path=str(tmp_path / "no_system"))
        capture_id = capture.start_capture("test-agent")
        conversation_id = "conv_no_system"

        # æ²¡æœ‰ç³»ç»Ÿpromptçš„å¯¹è¯
        for turn in range(1, 4):
            req_id = capture.capture_request(
                capture_id,
                {
                    "model": "test-model",
                    "prompt": f"User message {turn}"
                    # æ³¨æ„ï¼šæ²¡æœ‰system_promptå­—æ®µ
                },
                conversation_id=conversation_id,
                turn_number=turn
            )
            capture.capture_response(
                capture_id,
                {"text": f"Assistant response {turn}"},
                request_id=req_id
            )
            print(f"   Turn {turn}: å®Œæˆ")

        # éªŒè¯
        conv = capture.get_conversation_history(conversation_id)
        assert conv.system_prompt is None, "ä¸åº”è¯¥æœ‰ç³»ç»Ÿprompt"
        assert conv.total_turns == 3
        # åº”è¯¥åªæœ‰userå’Œassistantæ¶ˆæ¯ï¼Œæ²¡æœ‰systemæ¶ˆæ¯
        assert len(conv.messages) == 6  # 3 user + 3 assistant

        system_messages = conv.get_messages_by_role(MessageRole.SYSTEM)
        assert len(system_messages) == 0, "ä¸åº”è¯¥æœ‰systemæ¶ˆæ¯"

        print(f"\nâœ“ éªŒè¯ç»“æœ:")
        print(f"  â€¢ ç³»ç»ŸPrompt: None âœ“")
        print(f"  â€¢ æ¶ˆæ¯æ€»æ•°: {len(conv.messages)} (ä»…user+assistant) âœ“")
        print(f"  â€¢ Systemæ¶ˆæ¯æ•°: {len(system_messages)} âœ“")

        print("\nğŸ‰ å‘åå…¼å®¹æ€§æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
