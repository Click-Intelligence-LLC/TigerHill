"""
Prompt Analyzer - è‡ªåŠ¨åˆ†ææ•è·çš„ Prompt

æä¾›ä»¥ä¸‹åˆ†æåŠŸèƒ½ï¼š
- Token ä½¿ç”¨åˆ†æ
- Prompt è´¨é‡è¯„ä¼°
- æ€§èƒ½åˆ†æ
- å·¥å…·ä½¿ç”¨åˆ†æ
- ä¼˜åŒ–å»ºè®®
"""

from __future__ import annotations

import re
from typing import Any, Dict, List, Optional
from collections import Counter
import logging

logger = logging.getLogger(__name__)


class PromptAnalyzer:
    """
    Prompt è‡ªåŠ¨åˆ†æå™¨

    åˆ†ææ•è·çš„ prompt æ•°æ®ï¼Œæä¾›æ´å¯Ÿå’Œä¼˜åŒ–å»ºè®®ã€‚
    """

    def __init__(self, capture: Any):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            capture: PromptCapture å®ä¾‹æˆ–æ•è·æ•°æ®å­—å…¸
        """
        if hasattr(capture, "captures"):
            # PromptCapture å®ä¾‹
            self.captures = list(capture.captures.values())
        elif isinstance(capture, dict):
            # å•ä¸ªæ•è·æ•°æ®
            self.captures = [capture]
        elif isinstance(capture, list):
            # æ•è·æ•°æ®åˆ—è¡¨
            self.captures = capture
        else:
            raise ValueError("Invalid capture data")

        logger.info(f"Initialized analyzer with {len(self.captures)} captures")

    def analyze_all(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´åˆ†æ

        Returns:
            å®Œæ•´çš„åˆ†ææŠ¥å‘Š
        """
        report = {
            "summary": self.get_summary(),
            "token_analysis": self.analyze_tokens(),
            "prompt_quality": self.analyze_prompt_quality(),
            "performance": self.analyze_performance(),
            "tool_usage": self.analyze_tool_usage(),
            "recommendations": self.generate_recommendations()
        }

        return report

    def get_summary(self) -> Dict[str, Any]:
        """è·å–æ‘˜è¦ä¿¡æ¯"""
        total_captures = len(self.captures)
        total_requests = sum(len(c.get("requests", [])) for c in self.captures)
        total_responses = sum(len(c.get("responses", [])) for c in self.captures)

        # ç»Ÿè®¡ä¸åŒçš„ agent
        agents = set(c.get("agent_name") for c in self.captures)

        # ç»Ÿè®¡ä¸åŒçš„æ¨¡å‹
        models = set()
        for capture in self.captures:
            for request in capture.get("requests", []):
                if "model" in request:
                    models.add(request["model"])

        return {
            "total_captures": total_captures,
            "total_requests": total_requests,
            "total_responses": total_responses,
            "unique_agents": len(agents),
            "unique_models": len(models),
            "agents": list(agents),
            "models": list(models)
        }

    def analyze_tokens(self) -> Dict[str, Any]:
        """
        åˆ†æ Token ä½¿ç”¨æƒ…å†µ

        Returns:
            Token åˆ†ææŠ¥å‘Š
        """
        total_prompt_tokens = 0
        total_completion_tokens = 0
        total_tokens = 0

        token_counts = []

        for capture in self.captures:
            for response in capture.get("responses", []):
                usage = response.get("usage", {})
                prompt_tokens = usage.get("prompt_tokens", 0)
                completion_tokens = usage.get("completion_tokens", 0)

                total_prompt_tokens += prompt_tokens
                total_completion_tokens += completion_tokens
                total_tokens += prompt_tokens + completion_tokens

                if prompt_tokens + completion_tokens > 0:
                    token_counts.append(prompt_tokens + completion_tokens)

        # è®¡ç®—å¹³å‡å€¼
        avg_tokens = total_tokens / len(token_counts) if token_counts else 0
        avg_prompt_tokens = total_prompt_tokens / len(token_counts) if token_counts else 0
        avg_completion_tokens = total_completion_tokens / len(token_counts) if token_counts else 0

        # è®¡ç®— token æ•ˆç‡ï¼ˆcompletion / prompt æ¯”ç‡ï¼‰
        efficiency = (
            total_completion_tokens / total_prompt_tokens
            if total_prompt_tokens > 0
            else 0
        )

        return {
            "total_tokens": total_tokens,
            "total_prompt_tokens": total_prompt_tokens,
            "total_completion_tokens": total_completion_tokens,
            "avg_tokens_per_request": avg_tokens,
            "avg_prompt_tokens": avg_prompt_tokens,
            "avg_completion_tokens": avg_completion_tokens,
            "token_efficiency_ratio": efficiency,
            "max_tokens": max(token_counts) if token_counts else 0,
            "min_tokens": min(token_counts) if token_counts else 0
        }

    def analyze_prompt_quality(self) -> Dict[str, Any]:
        """
        åˆ†æ Prompt è´¨é‡

        Returns:
            è´¨é‡åˆ†ææŠ¥å‘Š
        """
        prompts = []
        system_prompts = []

        for capture in self.captures:
            for request in capture.get("requests", []):
                if "prompt" in request:
                    prompts.append(request["prompt"])
                if "system_prompt" in request:
                    system_prompts.append(request["system_prompt"])

        # åˆ†æ prompt é•¿åº¦
        prompt_lengths = [len(str(p)) for p in prompts]
        avg_prompt_length = sum(prompt_lengths) / len(prompt_lengths) if prompt_lengths else 0

        # åˆ†ææ˜¯å¦åŒ…å«ç³»ç»Ÿ prompt
        has_system_prompt_ratio = len(system_prompts) / len(prompts) if prompts else 0

        # æ£€æµ‹å¸¸è§é—®é¢˜
        issues = self._detect_prompt_issues(prompts)

        # åˆ†ææŒ‡ä»¤æ¸…æ™°åº¦
        clarity_score = self._calculate_clarity_score(prompts)

        return {
            "total_prompts": len(prompts),
            "avg_prompt_length": avg_prompt_length,
            "has_system_prompt_ratio": has_system_prompt_ratio,
            "clarity_score": clarity_score,
            "detected_issues": issues
        }

    def analyze_performance(self) -> Dict[str, Any]:
        """
        åˆ†ææ€§èƒ½æŒ‡æ ‡

        Returns:
            æ€§èƒ½åˆ†ææŠ¥å‘Š
        """
        durations = []

        for capture in self.captures:
            if "duration" in capture:
                durations.append(capture["duration"])

        if not durations:
            return {
                "avg_duration": 0,
                "max_duration": 0,
                "min_duration": 0,
                "total_duration": 0
            }

        return {
            "avg_duration": sum(durations) / len(durations),
            "max_duration": max(durations),
            "min_duration": min(durations),
            "total_duration": sum(durations),
            "num_captures": len(durations)
        }

    def analyze_tool_usage(self) -> Dict[str, Any]:
        """
        åˆ†æå·¥å…·ä½¿ç”¨æƒ…å†µ

        Returns:
            å·¥å…·ä½¿ç”¨åˆ†ææŠ¥å‘Š
        """
        tool_calls = []
        tools_defined = []

        for capture in self.captures:
            # æ”¶é›†å®šä¹‰çš„å·¥å…·
            for request in capture.get("requests", []):
                if "tools" in request and request["tools"]:
                    tools_defined.extend(request["tools"])

            # æ”¶é›†å·¥å…·è°ƒç”¨
            for response in capture.get("responses", []):
                if "tool_calls" in response and response["tool_calls"]:
                    tool_calls.extend(response["tool_calls"])

        # ç»Ÿè®¡å·¥å…·åç§°
        tool_names_defined = [
            t.get("name", t.get("function_name", "unknown"))
            for t in tools_defined
            if isinstance(t, dict)
        ]

        tool_names_called = [
            tc.get("name", "unknown")
            for tc in tool_calls
            if isinstance(tc, dict)
        ]

        # è®¡æ•°
        tool_counts = Counter(tool_names_called)

        # è®¡ç®—å·¥å…·ä½¿ç”¨ç‡
        total_requests = sum(len(c.get("requests", [])) for c in self.captures)
        tool_usage_rate = len(tool_calls) / total_requests if total_requests > 0 else 0

        return {
            "total_tools_defined": len(tools_defined),
            "unique_tools_defined": len(set(tool_names_defined)),
            "total_tool_calls": len(tool_calls),
            "unique_tools_called": len(set(tool_names_called)),
            "tool_usage_rate": tool_usage_rate,
            "most_used_tools": tool_counts.most_common(5),
            "tools_defined_but_not_used": list(
                set(tool_names_defined) - set(tool_names_called)
            )
        }

    def generate_recommendations(self) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆä¼˜åŒ–å»ºè®®

        Returns:
            å»ºè®®åˆ—è¡¨
        """
        recommendations = []

        # Token ä½¿ç”¨å»ºè®®
        token_analysis = self.analyze_tokens()
        if token_analysis["avg_prompt_tokens"] > 2000:
            recommendations.append({
                "category": "token_optimization",
                "severity": "medium",
                "title": "Prompt è¿‡é•¿",
                "description": f"å¹³å‡ prompt tokens: {token_analysis['avg_prompt_tokens']:.0f}",
                "suggestion": "è€ƒè™‘ç®€åŒ– prompt æˆ–ä½¿ç”¨æ›´çŸ­çš„ç¤ºä¾‹ã€‚é•¿ prompt ä¼šå¢åŠ æˆæœ¬å’Œå»¶è¿Ÿã€‚"
            })

        if token_analysis["token_efficiency_ratio"] < 0.5:
            recommendations.append({
                "category": "token_optimization",
                "severity": "low",
                "title": "Token æ•ˆç‡è¾ƒä½",
                "description": f"è¾“å‡º/è¾“å…¥æ¯”ç‡: {token_analysis['token_efficiency_ratio']:.2f}",
                "suggestion": "è¾“å‡ºç›¸å¯¹è¾“å…¥è¾ƒå°‘ï¼Œè€ƒè™‘æ˜¯å¦å¯ä»¥ç®€åŒ– prompt æˆ–è¦æ±‚æ›´è¯¦ç»†çš„è¾“å‡ºã€‚"
            })

        # Prompt è´¨é‡å»ºè®®
        quality = self.analyze_prompt_quality()
        if quality["has_system_prompt_ratio"] < 0.5:
            recommendations.append({
                "category": "prompt_quality",
                "severity": "medium",
                "title": "ç¼ºå°‘ç³»ç»Ÿ Prompt",
                "description": f"åªæœ‰ {quality['has_system_prompt_ratio']*100:.1f}% çš„è¯·æ±‚åŒ…å«ç³»ç»Ÿ prompt",
                "suggestion": "æ·»åŠ ç³»ç»Ÿ prompt å¯ä»¥æä¾›æ›´å¥½çš„ä¸Šä¸‹æ–‡å’Œè¡Œä¸ºæ§åˆ¶ã€‚"
            })

        if quality["clarity_score"] < 0.6:
            recommendations.append({
                "category": "prompt_quality",
                "severity": "high",
                "title": "Prompt æ¸…æ™°åº¦ä¸è¶³",
                "description": f"æ¸…æ™°åº¦è¯„åˆ†: {quality['clarity_score']:.2f}/1.0",
                "suggestion": "ä½¿ç”¨æ›´æ˜ç¡®çš„æŒ‡ä»¤ï¼ŒåŒ…å«å…·ä½“çš„æ ¼å¼è¦æ±‚å’Œç¤ºä¾‹ã€‚"
            })

        # æ·»åŠ æ£€æµ‹åˆ°çš„é—®é¢˜
        for issue in quality.get("detected_issues", []):
            recommendations.append({
                "category": "prompt_quality",
                "severity": "medium",
                "title": issue["type"],
                "description": issue["description"],
                "suggestion": issue["suggestion"]
            })

        # å·¥å…·ä½¿ç”¨å»ºè®®
        tool_usage = self.analyze_tool_usage()
        if tool_usage["tools_defined_but_not_used"]:
            recommendations.append({
                "category": "tool_usage",
                "severity": "low",
                "title": "æœªä½¿ç”¨çš„å·¥å…·",
                "description": f"å®šä¹‰äº† {len(tool_usage['tools_defined_but_not_used'])} ä¸ªä»æœªè¢«è°ƒç”¨çš„å·¥å…·",
                "suggestion": f"è€ƒè™‘ç§»é™¤è¿™äº›å·¥å…·: {', '.join(tool_usage['tools_defined_but_not_used'][:3])}"
            })

        # æ€§èƒ½å»ºè®®
        performance = self.analyze_performance()
        if performance["avg_duration"] > 10:
            recommendations.append({
                "category": "performance",
                "severity": "high",
                "title": "å“åº”æ—¶é—´è¿‡é•¿",
                "description": f"å¹³å‡å“åº”æ—¶é—´: {performance['avg_duration']:.2f}s",
                "suggestion": "è€ƒè™‘ç®€åŒ– promptã€å‡å°‘è¾“å‡ºé•¿åº¦æˆ–ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ã€‚"
            })

        return recommendations

    def _detect_prompt_issues(self, prompts: List[str]) -> List[Dict[str, str]]:
        """æ£€æµ‹ prompt ä¸­çš„å¸¸è§é—®é¢˜"""
        issues = []

        for prompt in prompts:
            prompt_str = str(prompt)

            # æ£€æµ‹æ˜¯å¦è¿‡äºç®€çŸ­
            if len(prompt_str) < 20:
                issues.append({
                    "type": "too_short",
                    "description": "Prompt è¿‡äºç®€çŸ­ï¼Œå¯èƒ½å¯¼è‡´ä¸æ˜ç¡®çš„è¾“å‡º",
                    "suggestion": "æä¾›æ›´å¤šä¸Šä¸‹æ–‡å’Œå…·ä½“è¦æ±‚"
                })

            # æ£€æµ‹æ˜¯å¦ç¼ºå°‘æ˜ç¡®æŒ‡ä»¤
            if not any(word in prompt_str.lower() for word in ["please", "è¯·", "should", "å¿…é¡»", "éœ€è¦"]):
                if not re.search(r'[\.\?!]', prompt_str):
                    issues.append({
                        "type": "lacks_clear_instruction",
                        "description": "ç¼ºå°‘æ˜ç¡®çš„æŒ‡ä»¤æˆ–è¯·æ±‚",
                        "suggestion": "ä½¿ç”¨æ˜ç¡®çš„åŠ¨è¯å’Œè¦æ±‚ï¼Œå¦‚ 'è¯·ç”Ÿæˆ...'ã€'åˆ†æ...' ç­‰"
                    })

            # æ£€æµ‹æ˜¯å¦åŒ…å«ç¤ºä¾‹
            if "example" not in prompt_str.lower() and "ç¤ºä¾‹" not in prompt_str:
                if len(prompt_str) > 200:  # åªå¯¹å¤æ‚ä»»åŠ¡å»ºè®®
                    issues.append({
                        "type": "lacks_examples",
                        "description": "å¤æ‚ä»»åŠ¡ç¼ºå°‘ç¤ºä¾‹",
                        "suggestion": "æä¾›è¾“å…¥/è¾“å‡ºç¤ºä¾‹å¯ä»¥æ˜¾è‘—æé«˜è¾“å‡ºè´¨é‡"
                    })

        return issues[:5]  # æœ€å¤šè¿”å› 5 ä¸ªé—®é¢˜

    def _calculate_clarity_score(self, prompts: List[str]) -> float:
        """
        è®¡ç®— prompt æ¸…æ™°åº¦è¯„åˆ†

        åŸºäºä»¥ä¸‹å› ç´ ï¼š
        - æ˜¯å¦åŒ…å«æ˜ç¡®çš„æŒ‡ä»¤åŠ¨è¯
        - æ˜¯å¦æœ‰æ ¼å¼è¦æ±‚
        - æ˜¯å¦æœ‰ç¤ºä¾‹
        - æ˜¯å¦æœ‰çº¦æŸæ¡ä»¶
        """
        if not prompts:
            return 0.0

        scores = []

        for prompt in prompts:
            prompt_str = str(prompt).lower()
            score = 0.0

            # åŒ…å«æŒ‡ä»¤åŠ¨è¯ (+0.3)
            instruction_verbs = ['generate', 'create', 'analyze', 'summarize', 'explain',
                                 'ç”Ÿæˆ', 'åˆ›å»º', 'åˆ†æ', 'æ€»ç»“', 'è§£é‡Š']
            if any(verb in prompt_str for verb in instruction_verbs):
                score += 0.3

            # åŒ…å«æ ¼å¼è¦æ±‚ (+0.2)
            if any(word in prompt_str for word in ['format', 'structure', 'json', 'markdown',
                                                     'æ ¼å¼', 'ç»“æ„']):
                score += 0.2

            # åŒ…å«ç¤ºä¾‹ (+0.3)
            if any(word in prompt_str for word in ['example', 'for instance', 'such as',
                                                     'ç¤ºä¾‹', 'ä¾‹å¦‚']):
                score += 0.3

            # åŒ…å«çº¦æŸ (+0.2)
            if any(word in prompt_str for word in ['must', 'should', 'limit', 'maximum',
                                                     'å¿…é¡»', 'åº”è¯¥', 'é™åˆ¶', 'æœ€å¤š']):
                score += 0.2

            scores.append(min(score, 1.0))

        return sum(scores) / len(scores)

    def print_report(self, report: Optional[Dict[str, Any]] = None) -> None:
        """
        æ‰“å°åˆ†ææŠ¥å‘Š

        Args:
            report: åˆ†ææŠ¥å‘Šï¼Œå¦‚æœä¸º None åˆ™æ‰§è¡Œå®Œæ•´åˆ†æ
        """
        if report is None:
            report = self.analyze_all()

        print("\n" + "=" * 80)
        print("ğŸ“Š TigerHill Prompt Analysis Report")
        print("=" * 80)

        # æ‘˜è¦
        summary = report["summary"]
        print("\nğŸ“‹ Summary:")
        print(f"   Total Captures: {summary['total_captures']}")
        print(f"   Total Requests: {summary['total_requests']}")
        print(f"   Total Responses: {summary['total_responses']}")
        print(f"   Agents: {', '.join(summary['agents'])}")
        print(f"   Models: {', '.join(summary['models'])}")

        # Token åˆ†æ
        tokens = report["token_analysis"]
        print("\nğŸ’° Token Usage:")
        print(f"   Total Tokens: {tokens['total_tokens']:,}")
        print(f"   Average per Request: {tokens['avg_tokens_per_request']:.0f}")
        print(f"   Efficiency Ratio: {tokens['token_efficiency_ratio']:.2f}")

        # è´¨é‡åˆ†æ
        quality = report["prompt_quality"]
        print("\nâœ¨ Prompt Quality:")
        print(f"   Clarity Score: {quality['clarity_score']:.2f}/1.0")
        print(f"   Has System Prompt: {quality['has_system_prompt_ratio']*100:.1f}%")
        print(f"   Average Length: {quality['avg_prompt_length']:.0f} chars")

        # æ€§èƒ½
        perf = report["performance"]
        print("\nâš¡ Performance:")
        print(f"   Average Duration: {perf['avg_duration']:.2f}s")
        print(f"   Max Duration: {perf['max_duration']:.2f}s")

        # å·¥å…·ä½¿ç”¨
        tools = report["tool_usage"]
        print("\nğŸ› ï¸  Tool Usage:")
        print(f"   Total Calls: {tools['total_tool_calls']}")
        print(f"   Usage Rate: {tools['tool_usage_rate']*100:.1f}%")
        if tools['most_used_tools']:
            print(f"   Most Used: {tools['most_used_tools'][0][0]} ({tools['most_used_tools'][0][1]} calls)")

        # å»ºè®®
        recommendations = report["recommendations"]
        if recommendations:
            print("\nğŸ’¡ Recommendations:")
            for i, rec in enumerate(recommendations[:5], 1):
                severity_emoji = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(rec["severity"], "âšª")
                print(f"\n   [{i}] {severity_emoji} {rec['title']}")
                print(f"       {rec['description']}")
                print(f"       ğŸ’¬ {rec['suggestion']}")

        print("\n" + "=" * 80)
