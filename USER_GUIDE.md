# ğŸ¯ TigerHillï¼ˆè™ä¸˜ï¼‰ä½¿ç”¨æ‰‹å†Œ

**AI Agent æµ‹è¯•ã€è¯„ä¼°å’Œè°ƒè¯•å®Œå…¨æŒ‡å—**

---

## ğŸ“š ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
3. [åŸºç¡€å·¥ä½œæµ](#åŸºç¡€å·¥ä½œæµ)
4. [æµ‹è¯• Agent](#æµ‹è¯•-agent)
5. [è¯„ä¼° Agent](#è¯„ä¼°-agent)
6. [è°ƒè¯• Agent](#è°ƒè¯•-agent)
7. [ä½¿ç”¨ AgentBay](#ä½¿ç”¨-agentbay)
8. [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
9. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
10. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
cd /path/to/TigerHill

# å®‰è£…ä¾èµ–
pip install -e ".[dev]"

# å®‰è£… AgentBay SDKï¼ˆå¯é€‰ï¼‰
pip install wuying-agentbay-sdk

# è®¾ç½® API Keyï¼ˆå¦‚æœä½¿ç”¨ AgentBayï¼‰
export AGENTBAY_API_KEY=your_api_key_here
```

### 5 åˆ†é’Ÿå¿«é€Ÿä½“éªŒ

```python
from tigerhill.storage.trace_store import TraceStore
from tigerhill.core.models import Task
from tigerhill.eval.assertions import run_assertions

# 1. åˆ›å»ºè¿½è¸ªå­˜å‚¨
store = TraceStore(storage_path="./my_traces")

# 2. å®šä¹‰æµ‹è¯•ä»»åŠ¡
task = Task(
    prompt="è®¡ç®— 6 + 7 çš„ç»“æœ",
    assertions=[
        {"type": "contains", "expected": "13"},
        {"type": "regex", "pattern": r"\d+"}
    ]
)

# 3. å¼€å§‹è¿½è¸ª
trace_id = store.start_trace(agent_name="calculator_agent", task_id="test_001")

# 4. è¿è¡Œä½ çš„ Agentï¼ˆè¿™é‡Œç”¨æ¨¡æ‹Ÿè¾“å‡ºï¼‰
agent_output = "è®¡ç®—ç»“æœæ˜¯ 13"

# 5. è®°å½•æ‰§è¡Œè¿‡ç¨‹
store.write_event({"type": "prompt", "content": task.prompt})
store.write_event({"type": "model_response", "text": agent_output})

# 6. ç»“æŸè¿½è¸ª
store.end_trace(trace_id)

# 7. è¯„ä¼°ç»“æœ
results = run_assertions(agent_output, task.assertions)

# 8. æŸ¥çœ‹ç»“æœ
print(f"é€šè¿‡: {sum(1 for r in results if r['ok'])}/{len(results)}")
print(f"è¿½è¸ªå·²ä¿å­˜åˆ°: {store.storage_path}")
```

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1. Taskï¼ˆä»»åŠ¡ï¼‰

ä»»åŠ¡å®šä¹‰äº†è¦æµ‹è¯•çš„å†…å®¹å’ŒæœŸæœ›çš„ç»“æœã€‚

```python
from tigerhill.core.models import Task

task = Task(
    prompt="ç”¨æˆ·çš„è¾“å…¥æç¤ºè¯",
    setup=["setup_step_1", "setup_step_2"],  # å¯é€‰çš„å‡†å¤‡æ­¥éª¤
    assertions=[                              # æ–­è¨€åˆ—è¡¨
        {"type": "contains", "expected": "æœŸæœ›çš„å†…å®¹"}
    ]
)
```

**å­—æ®µè¯´æ˜**:
- `prompt`: Agent éœ€è¦å¤„ç†çš„è¾“å…¥
- `setup`: æµ‹è¯•å‰çš„å‡†å¤‡æ­¥éª¤ï¼ˆå¯é€‰ï¼‰
- `assertions`: ç”¨äºéªŒè¯è¾“å‡ºçš„æ–­è¨€åˆ—è¡¨

---

### 2. Environmentï¼ˆç¯å¢ƒï¼‰

ç¯å¢ƒå®šä¹‰äº† Agent è¿è¡Œçš„ä¸Šä¸‹æ–‡ã€‚

```python
from tigerhill.core.models import Environment

env = Environment(
    name="æµ‹è¯•ç¯å¢ƒ",
    agentbay_env_id="codespace",        # AgentBay ç¯å¢ƒç±»å‹
    agentbay_tool_set_id="command"      # AgentBay å·¥å…·é›†
)
```

**AgentBay ç¯å¢ƒç±»å‹**:
- `codespace`: ä»£ç æ‰§è¡Œç¯å¢ƒ
- `browser`: æµè§ˆå™¨ç¯å¢ƒ
- `computer`: æ¡Œé¢ç¯å¢ƒ
- `mobile`: ç§»åŠ¨ç¯å¢ƒ

---

### 3. TraceStoreï¼ˆè¿½è¸ªå­˜å‚¨ï¼‰

TraceStore è®°å½• Agent æ‰§è¡Œçš„æ‰€æœ‰ç»†èŠ‚ã€‚

```python
from tigerhill.storage.trace_store import TraceStore

store = TraceStore(
    storage_path="./traces",  # å­˜å‚¨è·¯å¾„
    auto_save=True            # è‡ªåŠ¨ä¿å­˜
)
```

**è¿½è¸ªç”Ÿå‘½å‘¨æœŸ**:
```
å¼€å§‹è¿½è¸ª â†’ è®°å½•äº‹ä»¶ â†’ ç»“æŸè¿½è¸ª â†’ æŸ¥è¯¢åˆ†æ
```

---

### 4. Assertionsï¼ˆæ–­è¨€ï¼‰

æ–­è¨€ç”¨äºéªŒè¯ Agent çš„è¾“å‡ºæ˜¯å¦ç¬¦åˆé¢„æœŸã€‚

**æ”¯æŒçš„æ–­è¨€ç±»å‹**:

| ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `contains` | åŒ…å«æ£€æŸ¥ | `{"type": "contains", "expected": "13"}` |
| `equals` | ç²¾ç¡®åŒ¹é… | `{"type": "equals", "expected": "result: 13"}` |
| `regex` | æ­£åˆ™åŒ¹é… | `{"type": "regex", "pattern": r"\d+"}` |
| `starts_with` | å‰ç¼€åŒ¹é… | `{"type": "starts_with", "expected": "ç»“æœæ˜¯"}` |
| `ends_with` | åç¼€åŒ¹é… | `{"type": "ends_with", "expected": "å®Œæˆ"}` |
| `negate` | å¦å®šæ–­è¨€ | `{"type": "contains", "expected": "é”™è¯¯", "negate": true}` |

---

## åŸºç¡€å·¥ä½œæµ

### å®Œæ•´çš„æµ‹è¯•æµç¨‹

```python
from tigerhill.storage.trace_store import TraceStore
from tigerhill.core.models import Task, Environment
from tigerhill.eval.assertions import run_assertions

# ============================================
# ç¬¬ 1 æ­¥ï¼šå‡†å¤‡å·¥ä½œ
# ============================================

# åˆ›å»ºè¿½è¸ªå­˜å‚¨
store = TraceStore(storage_path="./agent_traces")

# å®šä¹‰æµ‹è¯•ä»»åŠ¡
task = Task(
    prompt="å†™ä¸€ä¸ªå‡½æ•°è®¡ç®—ä¸¤ä¸ªæ•°çš„å’Œ",
    assertions=[
        {"type": "contains", "expected": "def"},
        {"type": "contains", "expected": "return"},
        {"type": "regex", "pattern": r"def\s+\w+\s*\("}
    ]
)

# ============================================
# ç¬¬ 2 æ­¥ï¼šå¼€å§‹è¿½è¸ª
# ============================================

trace_id = store.start_trace(
    agent_name="code_generator",
    task_id="function_test_001",
    metadata={
        "version": "1.0",
        "model": "gpt-4",
        "temperature": 0.7
    }
)

print(f"å¼€å§‹è¿½è¸ª: {trace_id}")

# ============================================
# ç¬¬ 3 æ­¥ï¼šæ‰§è¡Œ Agent
# ============================================

# è®°å½•è¾“å…¥
store.write_event({
    "type": "prompt",
    "messages": [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹"},
        {"role": "user", "content": task.prompt}
    ]
})

# è¿™é‡Œè°ƒç”¨ä½ çš„ Agent
# agent_output = your_agent.run(task.prompt)
# ç¤ºä¾‹è¾“å‡º
agent_output = """
def add(a, b):
    return a + b
"""

# è®°å½•è¾“å‡º
store.write_event({
    "type": "model_response",
    "text": agent_output,
    "tool_calls": []
})

# ============================================
# ç¬¬ 4 æ­¥ï¼šç»“æŸè¿½è¸ª
# ============================================

store.end_trace(trace_id)

# ============================================
# ç¬¬ 5 æ­¥ï¼šè¯„ä¼°ç»“æœ
# ============================================

results = run_assertions(agent_output, task.assertions)

# æ‰“å°è¯„ä¼°ç»“æœ
print("\nè¯„ä¼°ç»“æœ:")
for i, result in enumerate(results, 1):
    status = "âœ… é€šè¿‡" if result["ok"] else "âŒ å¤±è´¥"
    print(f"{i}. {status} - {result['type']}")
    if not result["ok"]:
        print(f"   åŸå› : {result['message']}")

# ============================================
# ç¬¬ 6 æ­¥ï¼šæŸ¥çœ‹è¿½è¸ªæ‘˜è¦
# ============================================

summary = store.get_summary(trace_id)
print(f"\nè¿½è¸ªæ‘˜è¦:")
print(f"- Agent: {summary['agent_name']}")
print(f"- è€—æ—¶: {summary['duration_seconds']:.2f} ç§’")
print(f"- äº‹ä»¶æ•°: {summary['total_events']}")
print(f"- äº‹ä»¶ç»Ÿè®¡: {summary['event_counts']}")

# ============================================
# ç¬¬ 7 æ­¥ï¼šå¯¼å‡ºè¿½è¸ªï¼ˆå¯é€‰ï¼‰
# ============================================

store.export_trace(trace_id, f"./reports/trace_{trace_id}.json")
print(f"\nè¿½è¸ªå·²å¯¼å‡º")
```

---

## æµ‹è¯• Agent

### 1. å•æ¬¡æµ‹è¯•

æµ‹è¯• Agent å¯¹å•ä¸ªè¾“å…¥çš„å“åº”ã€‚

```python
from tigerhill.storage.trace_store import TraceStore
from tigerhill.core.models import Task
from tigerhill.eval.assertions import run_assertions

def test_single_input():
    """æµ‹è¯•å•ä¸ªè¾“å…¥"""

    # åˆ›å»ºå­˜å‚¨
    store = TraceStore()

    # å®šä¹‰ä»»åŠ¡
    task = Task(
        prompt="ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ",
        assertions=[
            {"type": "contains", "expected": "ç¼–ç¨‹è¯­è¨€"},
            {"type": "contains", "expected": "Python"}
        ]
    )

    # è¿½è¸ªå’Œæ‰§è¡Œ
    trace_id = store.start_trace(agent_name="qa_agent", task_id="single_test")

    # è°ƒç”¨ä½ çš„ Agent
    output = your_agent.query(task.prompt)

    # è®°å½•
    store.write_event({"type": "prompt", "content": task.prompt})
    store.write_event({"type": "model_response", "text": output})
    store.end_trace(trace_id)

    # è¯„ä¼°
    results = run_assertions(output, task.assertions)

    # è¿”å›ç»“æœ
    return {
        "trace_id": trace_id,
        "passed": all(r["ok"] for r in results),
        "results": results
    }

# è¿è¡Œæµ‹è¯•
result = test_single_input()
print(f"æµ‹è¯•é€šè¿‡: {result['passed']}")
```

---

### 2. æ‰¹é‡æµ‹è¯•

æµ‹è¯• Agent å¯¹å¤šä¸ªè¾“å…¥çš„å“åº”ã€‚

```python
from tigerhill.storage.trace_store import TraceStore
from tigerhill.core.models import Task
from tigerhill.eval.assertions import run_assertions

def test_batch_inputs():
    """æ‰¹é‡æµ‹è¯•å¤šä¸ªè¾“å…¥"""

    store = TraceStore(storage_path="./batch_tests")

    # å®šä¹‰æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "prompt": "2 + 2 = ?",
            "assertions": [{"type": "contains", "expected": "4"}]
        },
        {
            "prompt": "10 - 3 = ?",
            "assertions": [{"type": "contains", "expected": "7"}]
        },
        {
            "prompt": "5 Ã— 6 = ?",
            "assertions": [{"type": "contains", "expected": "30"}]
        }
    ]

    results = []

    for i, test_case in enumerate(test_cases):
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}/{len(test_cases)}")

        # åˆ›å»ºä»»åŠ¡
        task = Task(
            prompt=test_case["prompt"],
            assertions=test_case["assertions"]
        )

        # è¿½è¸ª
        trace_id = store.start_trace(
            agent_name="math_agent",
            task_id=f"batch_test_{i+1}"
        )

        # æ‰§è¡Œ
        output = your_agent.query(task.prompt)

        # è®°å½•
        store.write_event({"type": "prompt", "content": task.prompt})
        store.write_event({"type": "model_response", "text": output})
        store.end_trace(trace_id)

        # è¯„ä¼°
        assertion_results = run_assertions(output, task.assertions)
        passed = all(r["ok"] for r in assertion_results)

        results.append({
            "test_case": i+1,
            "prompt": task.prompt,
            "output": output,
            "passed": passed,
            "trace_id": trace_id
        })

        print(f"  {'âœ… é€šè¿‡' if passed else 'âŒ å¤±è´¥'}")

    # ç»Ÿè®¡
    total = len(results)
    passed = sum(1 for r in results if r["passed"])

    print(f"\næ‰¹é‡æµ‹è¯•å®Œæˆ:")
    print(f"  æ€»æ•°: {total}")
    print(f"  é€šè¿‡: {passed}")
    print(f"  å¤±è´¥: {total - passed}")
    print(f"  é€šè¿‡ç‡: {passed/total*100:.1f}%")

    return results

# è¿è¡Œæ‰¹é‡æµ‹è¯•
batch_results = test_batch_inputs()
```

---

### 3. å›å½’æµ‹è¯•

ç¡®ä¿ Agent çš„æ”¹è¿›ä¸ä¼šç ´åå·²æœ‰åŠŸèƒ½ã€‚

```python
import json
from pathlib import Path
from tigerhill.storage.trace_store import TraceStore
from tigerhill.core.models import Task
from tigerhill.eval.assertions import run_assertions

def create_test_suite(name: str, test_cases: list):
    """åˆ›å»ºæµ‹è¯•å¥—ä»¶"""
    suite_path = Path(f"./test_suites/{name}.json")
    suite_path.parent.mkdir(parents=True, exist_ok=True)

    with open(suite_path, 'w', encoding='utf-8') as f:
        json.dump(test_cases, f, indent=2, ensure_ascii=False)

    print(f"æµ‹è¯•å¥—ä»¶å·²åˆ›å»º: {suite_path}")

def run_regression_tests(suite_name: str, agent_version: str):
    """è¿è¡Œå›å½’æµ‹è¯•"""

    # åŠ è½½æµ‹è¯•å¥—ä»¶
    suite_path = Path(f"./test_suites/{suite_name}.json")
    with open(suite_path, 'r', encoding='utf-8') as f:
        test_cases = json.load(f)

    store = TraceStore(storage_path=f"./regression/{agent_version}")

    print(f"\nå¼€å§‹å›å½’æµ‹è¯•: {suite_name}")
    print(f"Agent ç‰ˆæœ¬: {agent_version}")
    print(f"æµ‹è¯•ç”¨ä¾‹æ•°: {len(test_cases)}\n")

    results = []

    for i, test_case in enumerate(test_cases):
        task = Task(
            prompt=test_case["prompt"],
            assertions=test_case["assertions"]
        )

        trace_id = store.start_trace(
            agent_name=f"agent_{agent_version}",
            task_id=test_case.get("id", f"test_{i+1}"),
            metadata={"suite": suite_name, "version": agent_version}
        )

        # æ‰§è¡Œ Agent
        output = your_agent.query(task.prompt)

        # è®°å½•
        store.write_event({"type": "prompt", "content": task.prompt})
        store.write_event({"type": "model_response", "text": output})
        store.end_trace(trace_id)

        # è¯„ä¼°
        assertion_results = run_assertions(output, task.assertions)
        passed = all(r["ok"] for r in assertion_results)

        results.append({
            "test_id": test_case.get("id"),
            "passed": passed,
            "trace_id": trace_id
        })

        status = "âœ…" if passed else "âŒ"
        print(f"{status} æµ‹è¯• {i+1}: {test_case.get('id', 'unnamed')}")

    # ä¿å­˜ç»“æœ
    report_path = Path(f"./regression/{agent_version}/report.json")
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump({
            "suite": suite_name,
            "version": agent_version,
            "total": len(results),
            "passed": sum(1 for r in results if r["passed"]),
            "results": results
        }, f, indent=2)

    print(f"\nå›å½’æµ‹è¯•æŠ¥å‘Š: {report_path}")

    return results

# ä½¿ç”¨ç¤ºä¾‹
# 1. åˆ›å»ºæµ‹è¯•å¥—ä»¶
test_cases = [
    {
        "id": "math_add",
        "prompt": "è®¡ç®— 5 + 3",
        "assertions": [{"type": "contains", "expected": "8"}]
    },
    {
        "id": "math_multiply",
        "prompt": "è®¡ç®— 4 Ã— 7",
        "assertions": [{"type": "contains", "expected": "28"}]
    }
]

create_test_suite("math_suite", test_cases)

# 2. è¿è¡Œå›å½’æµ‹è¯•
run_regression_tests("math_suite", "v1.0")
run_regression_tests("math_suite", "v1.1")  # æ–°ç‰ˆæœ¬æµ‹è¯•
```

### 6. æµ‹è¯•é Python Agentï¼ˆè·¨è¯­è¨€æµ‹è¯•ï¼‰

TigerHill å¯ä»¥æµ‹è¯•**ä»»ä½•ç¼–ç¨‹è¯­è¨€**ç¼–å†™çš„ Agentã€‚é€šè¿‡é€‚é…å™¨æ¨¡å¼ï¼Œæ”¯æŒï¼š
- **HTTP/REST API** Agentï¼ˆNode.jsã€Goã€Java ç­‰ï¼‰
- **å‘½ä»¤è¡Œ CLI** Agentï¼ˆGoã€Rustã€C++ ç­‰ï¼‰
- **æ ‡å‡†è¾“å…¥è¾“å‡º** Agentï¼ˆJavaã€C# ç­‰ï¼‰
- **AgentBay äº‘ç¯å¢ƒ** Agentï¼ˆä»»ä½•è¯­è¨€ï¼‰

#### æµ‹è¯• HTTP Agentï¼ˆNode.js ç¤ºä¾‹ï¼‰

```python
from tigerhill.adapters import HTTPAgentAdapter, UniversalAgentTester
from tigerhill.storage.trace_store import TraceStore

def test_nodejs_agent():
    """æµ‹è¯• Node.js HTTP Agent"""

    # 1. åˆ›å»º HTTP é€‚é…å™¨
    adapter = HTTPAgentAdapter(
        base_url="http://localhost:3000",
        endpoint="/api/agent",
        timeout=30
    )

    # 2. åˆ›å»ºæµ‹è¯•å™¨
    store = TraceStore(storage_path="./traces/nodejs_agent")
    tester = UniversalAgentTester(adapter, store)

    # 3. æ‰§è¡Œæµ‹è¯•
    result = tester.test(
        task={
            "prompt": "è®¡ç®— 6 + 7",
            "assertions": [
                {"type": "contains", "expected": "13"}
            ]
        },
        agent_name="nodejs_calculator"
    )

    # 4. æŸ¥çœ‹ç»“æœ
    print(f"âœ… é€šè¿‡: {result['passed']}/{result['total']}")
    print(f"è¿½è¸ª ID: {result['trace_id']}")

    return result

# è¿è¡Œæµ‹è¯•
test_nodejs_agent()
```

#### æµ‹è¯• CLI Agentï¼ˆGo ç¤ºä¾‹ï¼‰

```python
from tigerhill.adapters import CLIAgentAdapter, UniversalAgentTester
from tigerhill.storage.trace_store import TraceStore

def test_go_agent():
    """æµ‹è¯• Go å‘½ä»¤è¡Œ Agent"""

    # 1. åˆ›å»º CLI é€‚é…å™¨
    adapter = CLIAgentAdapter(
        command="./go_agent",       # Go ç¼–è¯‘åçš„å¯æ‰§è¡Œæ–‡ä»¶
        args_template=["{prompt}"], # å‚æ•°æ¨¡æ¿
        timeout=10
    )

    # 2. åˆ›å»ºæµ‹è¯•å™¨
    store = TraceStore(storage_path="./traces/go_agent")
    tester = UniversalAgentTester(adapter, store)

    # 3. æ‰§è¡Œæµ‹è¯•
    result = tester.test(
        task={
            "prompt": "åˆ—å‡ºæ–‡ä»¶",
            "assertions": [
                {"type": "contains", "expected": "æ–‡ä»¶"}
            ]
        },
        agent_name="go_cli_agent"
    )

    print(f"âœ… é€šè¿‡: {result['passed']}/{result['total']}")

    return result

# è¿è¡Œæµ‹è¯•
test_go_agent()
```

#### æ‰¹é‡æµ‹è¯•å¤šè¯­è¨€ Agent

```python
from tigerhill.adapters import (
    HTTPAgentAdapter,
    CLIAgentAdapter,
    UniversalAgentTester
)
from tigerhill.storage.trace_store import TraceStore

def batch_test_multilang():
    """æ‰¹é‡æµ‹è¯•å¤šè¯­è¨€ Agent"""

    store = TraceStore(storage_path="./traces/multilang")

    # æµ‹è¯•é…ç½®
    test_configs = [
        {
            "name": "nodejs_agent",
            "adapter": HTTPAgentAdapter("http://localhost:3000"),
            "task": {
                "prompt": "è®¡ç®— 10 + 20",
                "assertions": [{"type": "contains", "expected": "30"}]
            }
        },
        {
            "name": "go_agent",
            "adapter": CLIAgentAdapter("./go_agent", ["{prompt}"]),
            "task": {
                "prompt": "åˆ—å‡ºæ–‡ä»¶",
                "assertions": [{"type": "contains", "expected": "æ–‡ä»¶"}]
            }
        }
    ]

    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    all_results = []
    for config in test_configs:
        tester = UniversalAgentTester(config["adapter"], store)
        result = tester.test(
            task=config["task"],
            agent_name=config["name"]
        )
        all_results.append(result)

        print(f"\n{config['name']}: {result['passed']}/{result['total']} é€šè¿‡")

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    report = tester.generate_report(all_results)
    print(f"\næ€»ä½“æˆåŠŸç‡: {report['success_rate']:.1f}%")

    return all_results

# è¿è¡Œæ‰¹é‡æµ‹è¯•
batch_test_multilang()
```

#### åœ¨ AgentBay æµ‹è¯•å¤šè¯­è¨€ Agent

```python
from tigerhill.agentbay.client import AgentBayClient, EnvironmentType
from tigerhill.adapters import AgentBayAdapter, UniversalAgentTester
from tigerhill.storage.trace_store import TraceStore

def test_multilang_in_agentbay():
    """åœ¨ AgentBay äº‘ç¯å¢ƒæµ‹è¯•å¤šè¯­è¨€ Agent"""

    store = TraceStore(storage_path="./traces/agentbay_multilang")

    with AgentBayClient() as client:
        # åˆ›å»ºäº‘ç¯å¢ƒä¼šè¯
        session = client.create_session(env_type=EnvironmentType.CODESPACE)
        session_id = session["session_id"]

        # æµ‹è¯• Node.js Agent
        print("æµ‹è¯• Node.js Agent...")
        node_adapter = AgentBayAdapter(
            client=client,
            session_id=session_id,
            agent_command="node agent.js '{prompt}'",
            setup_commands=[
                "apt-get update && apt-get install -y nodejs npm",
                "echo 'console.log(process.argv[2])' > agent.js"
            ]
        )

        tester = UniversalAgentTester(node_adapter, store)
        result = tester.test(
            task={"prompt": "æµ‹è¯•", "assertions": []},
            agent_name="nodejs_cloud_agent"
        )

        print(f"Node.js Agent: {'âœ…' if result['success'] else 'âŒ'}")

        # æµ‹è¯• Go Agent
        print("\næµ‹è¯• Go Agent...")
        go_adapter = AgentBayAdapter(
            client=client,
            session_id=session_id,
            agent_command="./agent '{prompt}'",
            setup_commands=[
                "apt-get update && apt-get install -y golang",
                "echo 'package main\nimport \"fmt\"\nimport \"os\"\nfunc main() { fmt.Println(os.Args[1]) }' > agent.go",
                "go build -o agent agent.go"
            ]
        )

        tester = UniversalAgentTester(go_adapter, store)
        result = tester.test(
            task={"prompt": "æµ‹è¯•", "assertions": []},
            agent_name="go_cloud_agent"
        )

        print(f"Go Agent: {'âœ…' if result['success'] else 'âŒ'}")

        # ä¼šè¯è‡ªåŠ¨æ¸…ç†

# è¿è¡Œæµ‹è¯•
test_multilang_in_agentbay()
```

**å®Œæ•´æ–‡æ¡£**: è¯¦è§ [CROSS_LANGUAGE_TESTING.md](CROSS_LANGUAGE_TESTING.md)

**ç¤ºä¾‹ä»£ç **: æŸ¥çœ‹ `examples/cross_language/` ç›®å½•ï¼š
- `test_nodejs_agent.py` - Node.js Agent æµ‹è¯•
- `test_go_agent.py` - Go Agent æµ‹è¯•
- `batch_test_multilang.py` - æ‰¹é‡å¤šè¯­è¨€æµ‹è¯•

---

## è¯„ä¼° Agent

### 1. åŸºç¡€è¯„ä¼°

ä½¿ç”¨æ–­è¨€è¯„ä¼° Agent è¾“å‡ºçš„è´¨é‡ã€‚

```python
from tigerhill.eval.assertions import run_assertions

def evaluate_agent_output(output: str, expected_criteria: dict):
    """è¯„ä¼° Agent è¾“å‡º"""

    # å®šä¹‰æ–­è¨€
    assertions = []

    # å†…å®¹æ£€æŸ¥
    if "required_keywords" in expected_criteria:
        for keyword in expected_criteria["required_keywords"]:
            assertions.append({
                "type": "contains",
                "expected": keyword
            })

    # æ ¼å¼æ£€æŸ¥
    if "format_pattern" in expected_criteria:
        assertions.append({
            "type": "regex",
            "pattern": expected_criteria["format_pattern"]
        })

    # ç¦æ­¢å†…å®¹
    if "forbidden_keywords" in expected_criteria:
        for keyword in expected_criteria["forbidden_keywords"]:
            assertions.append({
                "type": "contains",
                "expected": keyword,
                "negate": True  # å¦å®šæ–­è¨€
            })

    # è¿è¡Œæ–­è¨€
    results = run_assertions(output, assertions)

    # è®¡ç®—åˆ†æ•°
    total = len(results)
    passed = sum(1 for r in results if r["ok"])
    score = (passed / total * 100) if total > 0 else 0

    return {
        "score": score,
        "passed": passed,
        "total": total,
        "details": results
    }

# ä½¿ç”¨ç¤ºä¾‹
output = """
def calculate_sum(a, b):
    \"\"\"è®¡ç®—ä¸¤ä¸ªæ•°çš„å’Œ\"\"\"
    return a + b
"""

criteria = {
    "required_keywords": ["def", "return"],
    "format_pattern": r"def\s+\w+\s*\(",
    "forbidden_keywords": ["print", "input"]
}

evaluation = evaluate_agent_output(output, criteria)
print(f"è¯„ä¼°åˆ†æ•°: {evaluation['score']:.1f}%")
```

---

### 2. å¯¹æ¯”è¯„ä¼°

å¯¹æ¯”ä¸åŒç‰ˆæœ¬æˆ–ä¸åŒé…ç½®çš„ Agentã€‚

```python
from tigerhill.storage.trace_store import TraceStore
from tigerhill.eval.assertions import run_assertions

def compare_agents(test_cases: list, agents: dict):
    """å¯¹æ¯”å¤šä¸ª Agent çš„è¡¨ç°"""

    store = TraceStore(storage_path="./comparisons")

    comparison_results = {agent_name: [] for agent_name in agents.keys()}

    for i, test_case in enumerate(test_cases):
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}: {test_case['prompt']}")

        for agent_name, agent_func in agents.items():
            # è¿½è¸ª
            trace_id = store.start_trace(
                agent_name=agent_name,
                task_id=f"compare_test_{i+1}"
            )

            # æ‰§è¡Œ
            output = agent_func(test_case['prompt'])

            # è®°å½•
            store.write_event({"type": "prompt", "content": test_case['prompt']})
            store.write_event({"type": "model_response", "text": output})
            store.end_trace(trace_id)

            # è¯„ä¼°
            results = run_assertions(output, test_case['assertions'])
            passed = all(r["ok"] for r in results)

            # ç»Ÿè®¡
            comparison_results[agent_name].append({
                "test_id": i+1,
                "passed": passed,
                "output_length": len(output),
                "trace_id": trace_id
            })

            print(f"  {agent_name}: {'âœ… é€šè¿‡' if passed else 'âŒ å¤±è´¥'}")

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n" + "="*60)
    print("å¯¹æ¯”ç»“æœ:")
    print("="*60)

    for agent_name, results in comparison_results.items():
        total = len(results)
        passed = sum(1 for r in results if r["passed"])
        avg_length = sum(r["output_length"] for r in results) / total

        print(f"\n{agent_name}:")
        print(f"  é€šè¿‡ç‡: {passed}/{total} ({passed/total*100:.1f}%)")
        print(f"  å¹³å‡è¾“å‡ºé•¿åº¦: {avg_length:.0f} å­—ç¬¦")

    return comparison_results

# ä½¿ç”¨ç¤ºä¾‹
test_cases = [
    {
        "prompt": "è§£é‡Šä»€ä¹ˆæ˜¯é€’å½’",
        "assertions": [
            {"type": "contains", "expected": "å‡½æ•°"},
            {"type": "contains", "expected": "è‡ªå·±"}
        ]
    }
]

agents = {
    "agent_v1": lambda prompt: your_agent_v1.query(prompt),
    "agent_v2": lambda prompt: your_agent_v2.query(prompt),
    "agent_gpt4": lambda prompt: gpt4_agent.query(prompt)
}

comparison = compare_agents(test_cases, agents)
```

---

## è°ƒè¯• Agent

### 1. è¯¦ç»†è¿½è¸ª

è®°å½• Agent æ‰§è¡Œçš„æ¯ä¸€æ­¥ã€‚

```python
from tigerhill.storage.trace_store import TraceStore, EventType

def debug_agent_execution(prompt: str):
    """è¯¦ç»†è¿½è¸ª Agent æ‰§è¡Œè¿‡ç¨‹"""

    store = TraceStore(storage_path="./debug_traces")

    # å¼€å§‹è¿½è¸ª
    trace_id = store.start_trace(
        agent_name="debug_agent",
        task_id="debug_session",
        metadata={"debug_mode": True}
    )

    print(f"å¼€å§‹è°ƒè¯•è¿½è¸ª: {trace_id}\n")

    # 1. è®°å½•è¾“å…¥
    print("1ï¸âƒ£ ç”¨æˆ·è¾“å…¥:")
    print(f"   {prompt}")
    store.write_event({
        "type": "prompt",
        "content": prompt
    })

    # 2. è®°å½•ç³»ç»Ÿæç¤ºè¯
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"
    print(f"\n2ï¸âƒ£ ç³»ç»Ÿæç¤ºè¯:")
    print(f"   {system_prompt}")
    store.write_event({
        "type": "custom",
        "event": "system_prompt",
        "content": system_prompt
    })

    # 3. è°ƒç”¨ LLM
    print(f"\n3ï¸âƒ£ è°ƒç”¨ LLM...")
    # llm_response = your_llm.generate(prompt)
    llm_response = "è¿™æ˜¯ LLM çš„å“åº”"

    store.write_event({
        "type": "model_response",
        "text": llm_response,
        "metadata": {
            "model": "gpt-4",
            "temperature": 0.7,
            "tokens": 150
        }
    })
    print(f"   å“åº”: {llm_response}")

    # 4. å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
    tool_calls = []  # å‡è®¾ LLM å»ºè®®ä½¿ç”¨å·¥å…·
    if tool_calls:
        print(f"\n4ï¸âƒ£ å·¥å…·è°ƒç”¨:")
        for i, tool_call in enumerate(tool_calls):
            print(f"   {i+1}. {tool_call['name']}({tool_call['args']})")

            # æ‰§è¡Œå·¥å…·
            result = execute_tool(tool_call['name'], tool_call['args'])

            store.write_event({
                "type": "tool_call",
                "tool": tool_call['name'],
                "args": tool_call['args']
            })

            store.write_event({
                "type": "tool_result",
                "tool": tool_call['name'],
                "result": result
            })

            print(f"      ç»“æœ: {result}")

    # 5. æœ€ç»ˆè¾“å‡º
    final_output = llm_response  # æˆ–è€…æ˜¯å·¥å…·è°ƒç”¨åçš„ç»“æœ
    print(f"\n5ï¸âƒ£ æœ€ç»ˆè¾“å‡º:")
    print(f"   {final_output}")

    # ç»“æŸè¿½è¸ª
    store.end_trace(trace_id)

    # 6. åˆ†æè¿½è¸ª
    print(f"\n" + "="*60)
    print("è¿½è¸ªåˆ†æ:")
    print("="*60)

    summary = store.get_summary(trace_id)
    print(f"æ€»äº‹ä»¶æ•°: {summary['total_events']}")
    print(f"æ‰§è¡Œè€—æ—¶: {summary['duration_seconds']:.2f} ç§’")
    print(f"äº‹ä»¶åˆ†å¸ƒ:")
    for event_type, count in summary['event_counts'].items():
        print(f"  - {event_type}: {count}")

    # å¯¼å‡ºè¯¦ç»†è¿½è¸ª
    export_path = f"./debug_traces/debug_{trace_id}.json"
    store.export_trace(trace_id, export_path)
    print(f"\nè¯¦ç»†è¿½è¸ªå·²å¯¼å‡º: {export_path}")

    return trace_id

# ä½¿ç”¨
trace_id = debug_agent_execution("å†™ä¸€ä¸ªå†’æ³¡æ’åºç®—æ³•")
```

---

### 2. æ€§èƒ½åˆ†æ

åˆ†æ Agent çš„æ€§èƒ½ç“¶é¢ˆã€‚

```python
import time
from tigerhill.storage.trace_store import TraceStore

def profile_agent_performance(prompt: str, iterations: int = 10):
    """æ€§èƒ½åˆ†æ"""

    store = TraceStore(storage_path="./performance")

    print(f"å¼€å§‹æ€§èƒ½åˆ†æï¼ˆ{iterations} æ¬¡è¿­ä»£ï¼‰\n")

    timings = {
        "llm_call": [],
        "tool_execution": [],
        "total": []
    }

    for i in range(iterations):
        print(f"è¿­ä»£ {i+1}/{iterations}...")

        trace_id = store.start_trace(
            agent_name="perf_agent",
            task_id=f"perf_test_{i+1}"
        )

        start_time = time.time()

        # LLM è°ƒç”¨è®¡æ—¶
        llm_start = time.time()
        # llm_response = your_llm.generate(prompt)
        time.sleep(0.5)  # æ¨¡æ‹Ÿ LLM è°ƒç”¨
        llm_end = time.time()
        llm_time = llm_end - llm_start
        timings["llm_call"].append(llm_time)

        store.write_event({
            "type": "model_response",
            "text": "response",
            "metadata": {"latency_ms": llm_time * 1000}
        })

        # å·¥å…·æ‰§è¡Œè®¡æ—¶
        tool_start = time.time()
        # tool_result = execute_tool(...)
        time.sleep(0.1)  # æ¨¡æ‹Ÿå·¥å…·æ‰§è¡Œ
        tool_end = time.time()
        tool_time = tool_end - tool_start
        timings["tool_execution"].append(tool_time)

        store.write_event({
            "type": "tool_result",
            "tool": "example_tool",
            "result": "result",
            "metadata": {"latency_ms": tool_time * 1000}
        })

        end_time = time.time()
        total_time = end_time - start_time
        timings["total"].append(total_time)

        store.end_trace(trace_id)

    # åˆ†æç»“æœ
    print(f"\n" + "="*60)
    print("æ€§èƒ½åˆ†æç»“æœ:")
    print("="*60)

    for component, times in timings.items():
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)

        print(f"\n{component}:")
        print(f"  å¹³å‡: {avg_time*1000:.2f} ms")
        print(f"  æœ€å°: {min_time*1000:.2f} ms")
        print(f"  æœ€å¤§: {max_time*1000:.2f} ms")

    # ç“¶é¢ˆè¯†åˆ«
    avg_llm = sum(timings["llm_call"]) / len(timings["llm_call"])
    avg_tool = sum(timings["tool_execution"]) / len(timings["tool_execution"])
    avg_total = sum(timings["total"]) / len(timings["total"])

    llm_percent = (avg_llm / avg_total) * 100
    tool_percent = (avg_tool / avg_total) * 100

    print(f"\næ—¶é—´åˆ†å¸ƒ:")
    print(f"  LLM è°ƒç”¨: {llm_percent:.1f}%")
    print(f"  å·¥å…·æ‰§è¡Œ: {tool_percent:.1f}%")
    print(f"  å…¶ä»–: {100 - llm_percent - tool_percent:.1f}%")

    if llm_percent > 70:
        print(f"\nâš ï¸ ç“¶é¢ˆ: LLM è°ƒç”¨å ç”¨äº†å¤§éƒ¨åˆ†æ—¶é—´")
        print(f"   å»ºè®®: è€ƒè™‘ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹æˆ–ç¼“å­˜ç­–ç•¥")
    elif tool_percent > 70:
        print(f"\nâš ï¸ ç“¶é¢ˆ: å·¥å…·æ‰§è¡Œå ç”¨äº†å¤§éƒ¨åˆ†æ—¶é—´")
        print(f"   å»ºè®®: ä¼˜åŒ–å·¥å…·å®ç°æˆ–ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œ")

# ä½¿ç”¨
profile_agent_performance("æµ‹è¯•æç¤ºè¯", iterations=10)
```

---

### 3. é”™è¯¯è¿½è¸ª

è®°å½•å’Œåˆ†æ Agent çš„é”™è¯¯ã€‚

```python
from tigerhill.storage.trace_store import TraceStore
import traceback

def trace_agent_errors(prompt: str):
    """è¿½è¸ª Agent é”™è¯¯"""

    store = TraceStore(storage_path="./error_traces")

    trace_id = store.start_trace(
        agent_name="error_agent",
        task_id="error_test",
        metadata={"debug": True}
    )

    try:
        print("æ‰§è¡Œ Agent...")

        # è®°å½•è¾“å…¥
        store.write_event({
            "type": "prompt",
            "content": prompt
        })

        # æ‰§è¡Œ Agentï¼ˆå¯èƒ½å‡ºé”™ï¼‰
        # result = your_agent.run(prompt)

        # æ¨¡æ‹Ÿé”™è¯¯
        raise ValueError("ç¤ºä¾‹é”™è¯¯ï¼šè¾“å…¥æ ¼å¼ä¸æ­£ç¡®")

    except Exception as e:
        # è®°å½•é”™è¯¯
        error_info = {
            "type": "error",
            "error_type": type(e).__name__,
            "error_message": str(e),
            "traceback": traceback.format_exc()
        }

        store.write_event(error_info)

        print(f"âŒ é”™è¯¯: {e}")
        print(f"\nè¯¦ç»†è¿½è¸ª:")
        print(traceback.format_exc())

        # ä¿å­˜é”™è¯¯ä¸Šä¸‹æ–‡
        store.write_event({
            "type": "custom",
            "event": "error_context",
            "prompt": prompt,
            "metadata": {
                "error_occurred": True
            }
        })

    finally:
        store.end_trace(trace_id)

        # å¯¼å‡ºé”™è¯¯è¿½è¸ª
        error_trace_path = f"./error_traces/error_{trace_id}.json"
        store.export_trace(trace_id, error_trace_path)
        print(f"\né”™è¯¯è¿½è¸ªå·²ä¿å­˜: {error_trace_path}")

    return trace_id

# ä½¿ç”¨
trace_agent_errors("å¯¼è‡´é”™è¯¯çš„è¾“å…¥")
```

---

## ä½¿ç”¨ AgentBay

### 1. åŸºç¡€ä½¿ç”¨

åœ¨ AgentBay äº‘ç«¯ç¯å¢ƒä¸­æµ‹è¯• Agentã€‚

```python
from tigerhill.agentbay.client import AgentBayClient, EnvironmentType
from tigerhill.storage.trace_store import TraceStore

def test_agent_with_agentbay(prompt: str):
    """ä½¿ç”¨ AgentBay æµ‹è¯• Agent"""

    store = TraceStore(storage_path="./agentbay_tests")

    # åˆ›å»º AgentBay å®¢æˆ·ç«¯
    with AgentBayClient() as client:
        print("âœ… AgentBay å®¢æˆ·ç«¯å·²è¿æ¥")

        # åˆ›å»ºè¿½è¸ª
        trace_id = store.start_trace(
            agent_name="agentbay_agent",
            task_id="agentbay_test",
            metadata={"platform": "agentbay"}
        )

        # åˆ›å»ºäº‘ç«¯ä¼šè¯
        print("åˆ›å»º AgentBay ä¼šè¯...")
        session = client.create_session(env_type=EnvironmentType.CODESPACE)
        session_id = session["session_id"]
        print(f"âœ… ä¼šè¯å·²åˆ›å»º: {session_id}")

        # è®°å½•ä¼šè¯åˆ›å»º
        store.write_event({
            "type": "custom",
            "event": "agentbay_session_created",
            "session_id": session_id,
            "env_type": "codespace"
        })

        try:
            # æ‰§è¡Œå‘½ä»¤
            print(f"\næ‰§è¡Œå‘½ä»¤...")
            result = client.execute_command(
                session_id,
                "python -c 'print(\"Hello from AgentBay!\")'"
            )

            print(f"âœ… å‘½ä»¤è¾“å‡º: {result['output']}")

            # è®°å½•æ‰§è¡Œ
            store.write_event({
                "type": "tool_call",
                "tool": "execute_command",
                "args": {"command": "python -c '...'"}
            })

            store.write_event({
                "type": "tool_result",
                "tool": "execute_command",
                "result": result['output'],
                "exit_code": result['exit_code']
            })

        finally:
            # æ¸…ç†ä¼šè¯
            client.delete_session(session_id)
            print(f"âœ… ä¼šè¯å·²æ¸…ç†")

            # è®°å½•æ¸…ç†
            store.write_event({
                "type": "custom",
                "event": "agentbay_session_deleted",
                "session_id": session_id
            })

        # ç»“æŸè¿½è¸ª
        store.end_trace(trace_id)

        # æŸ¥çœ‹æ‘˜è¦
        summary = store.get_summary(trace_id)
        print(f"\nè¿½è¸ªæ‘˜è¦:")
        print(f"  æ€»äº‹ä»¶: {summary['total_events']}")
        print(f"  è€—æ—¶: {summary['duration_seconds']:.2f} ç§’")

        return trace_id

# ä½¿ç”¨ï¼ˆéœ€è¦è®¾ç½® AGENTBAY_API_KEYï¼‰
trace_id = test_agent_with_agentbay("æµ‹è¯•æç¤ºè¯")
```

---

### 2. å·¥å…·è°ƒç”¨æµ‹è¯•

æµ‹è¯• Agent ä½¿ç”¨ AgentBay å·¥å…·çš„èƒ½åŠ›ã€‚

```python
from tigerhill.agentbay.client import AgentBayClient, EnvironmentType
from tigerhill.storage.trace_store import TraceStore

def test_tool_usage():
    """æµ‹è¯•å·¥å…·è°ƒç”¨"""

    store = TraceStore()
    client = AgentBayClient()

    # åŠ è½½å¯ç”¨å·¥å…·
    print("åŠ è½½ AgentBay å·¥å…·...")
    tools = client.load_tools("command")
    print(f"âœ… å·²åŠ è½½ {len(tools)} ä¸ªå·¥å…·")
    for tool in tools:
        print(f"  - {tool['name']}: {tool['description']}")

    # åˆ›å»ºä¼šè¯
    session = client.create_session(env_type=EnvironmentType.CODESPACE)
    session_id = session["session_id"]

    # å¼€å§‹è¿½è¸ª
    trace_id = store.start_trace(agent_name="tool_test_agent")

    try:
        # æµ‹è¯•å¤šä¸ªå·¥å…·è°ƒç”¨
        test_commands = [
            "echo 'Test 1'",
            "python -c 'print(2 + 2)'",
            "ls /tmp"
        ]

        for i, cmd in enumerate(test_commands):
            print(f"\næµ‹è¯• {i+1}: {cmd}")

            # è®°å½•å·¥å…·è°ƒç”¨
            store.write_event({
                "type": "tool_call",
                "tool": "execute_command",
                "args": {"command": cmd},
                "index": i
            })

            # æ‰§è¡Œ
            result = client.execute_command(session_id, cmd)

            # è®°å½•ç»“æœ
            store.write_event({
                "type": "tool_result",
                "tool": "execute_command",
                "result": result['output'],
                "exit_code": result['exit_code'],
                "index": i
            })

            print(f"  è¾“å‡º: {result['output']}")
            print(f"  é€€å‡ºç : {result['exit_code']}")

    finally:
        client.delete_session(session_id)
        store.end_trace(trace_id)

    print(f"\nâœ… å·¥å…·æµ‹è¯•å®Œæˆ")
    return trace_id

# ä½¿ç”¨
test_tool_usage()
```

---

## é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰è¯„ä¼°å™¨

åˆ›å»ºè‡ªå·±çš„è¯„ä¼°é€»è¾‘ã€‚

```python
from tigerhill.eval.assertions import AssertionResult
from typing import List, Dict, Any

class CustomEvaluator:
    """è‡ªå®šä¹‰è¯„ä¼°å™¨"""

    def __init__(self, name: str):
        self.name = name

    def evaluate_length(self, output: str, min_length: int, max_length: int) -> AssertionResult:
        """è¯„ä¼°è¾“å‡ºé•¿åº¦"""
        length = len(output)
        ok = min_length <= length <= max_length

        return AssertionResult(
            type="length_check",
            ok=ok,
            expected=f"{min_length}-{max_length}",
            actual=length,
            message="" if ok else f"é•¿åº¦ {length} ä¸åœ¨èŒƒå›´å†…"
        )

    def evaluate_sentiment(self, output: str, expected_sentiment: str) -> AssertionResult:
        """è¯„ä¼°æƒ…æ„Ÿå€¾å‘"""
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼Œå®é™…åº”ç”¨å¯ä»¥ä½¿ç”¨ NLP æ¨¡å‹
        positive_keywords = ["å¥½", "æ£’", "ä¼˜ç§€", "å®Œç¾"]
        negative_keywords = ["å·®", "ç³Ÿ", "å¤±è´¥", "é”™è¯¯"]

        positive_count = sum(1 for kw in positive_keywords if kw in output)
        negative_count = sum(1 for kw in negative_keywords if kw in output)

        if expected_sentiment == "positive":
            ok = positive_count > negative_count
        elif expected_sentiment == "negative":
            ok = negative_count > positive_count
        else:
            ok = positive_count == negative_count

        return AssertionResult(
            type="sentiment_check",
            ok=ok,
            expected=expected_sentiment,
            actual=f"pos:{positive_count}, neg:{negative_count}",
            message="" if ok else "æƒ…æ„Ÿå€¾å‘ä¸åŒ¹é…"
        )

    def evaluate_code_quality(self, code: str) -> AssertionResult:
        """è¯„ä¼°ä»£ç è´¨é‡"""
        issues = []

        # æ£€æŸ¥æ–‡æ¡£å­—ç¬¦ä¸²
        if '"""' not in code and "'''" not in code:
            issues.append("ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²")

        # æ£€æŸ¥ç±»å‹æ³¨è§£
        if "->" not in code and ":" not in code:
            issues.append("ç¼ºå°‘ç±»å‹æ³¨è§£")

        # æ£€æŸ¥å‘½åè§„èŒƒ
        if any(c.isupper() for c in code.split("def ")[1].split("(")[0] if "def " in code):
            issues.append("å‡½æ•°ååº”ä½¿ç”¨å°å†™")

        ok = len(issues) == 0

        return AssertionResult(
            type="code_quality",
            ok=ok,
            expected="é«˜è´¨é‡ä»£ç ",
            actual=f"{len(issues)} ä¸ªé—®é¢˜",
            message="; ".join(issues) if issues else ""
        )

    def run_all_evaluations(self, output: str, criteria: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¿è¡Œæ‰€æœ‰è¯„ä¼°"""
        results = []

        # é•¿åº¦æ£€æŸ¥
        if "length" in criteria:
            result = self.evaluate_length(
                output,
                criteria["length"]["min"],
                criteria["length"]["max"]
            )
            results.append(result.to_dict())

        # æƒ…æ„Ÿæ£€æŸ¥
        if "sentiment" in criteria:
            result = self.evaluate_sentiment(
                output,
                criteria["sentiment"]
            )
            results.append(result.to_dict())

        # ä»£ç è´¨é‡æ£€æŸ¥
        if criteria.get("check_code_quality", False):
            result = self.evaluate_code_quality(output)
            results.append(result.to_dict())

        return results

# ä½¿ç”¨ç¤ºä¾‹
evaluator = CustomEvaluator("my_evaluator")

output = """
def calculate_sum(a, b):
    \"\"\"è®¡ç®—ä¸¤ä¸ªæ•°çš„å’Œ\"\"\"
    return a + b
"""

criteria = {
    "length": {"min": 50, "max": 200},
    "sentiment": "positive",
    "check_code_quality": True
}

results = evaluator.run_all_evaluations(output, criteria)

for result in results:
    status = "âœ…" if result["ok"] else "âŒ"
    print(f"{status} {result['type']}: {result['message'] or 'OK'}")
```

---

### 2. æ•°æ®é›†ç®¡ç†

ç»„ç»‡å’Œç®¡ç†æµ‹è¯•æ•°æ®é›†ã€‚

```python
import json
from pathlib import Path
from typing import List, Dict, Any

class DatasetManager:
    """æ•°æ®é›†ç®¡ç†å™¨"""

    def __init__(self, datasets_dir: str = "./datasets"):
        self.datasets_dir = Path(datasets_dir)
        self.datasets_dir.mkdir(parents=True, exist_ok=True)

    def create_dataset(self, name: str, data: List[Dict[str, Any]]):
        """åˆ›å»ºæ•°æ®é›†"""
        dataset_path = self.datasets_dir / f"{name}.json"

        with open(dataset_path, 'w', encoding='utf-8') as f:
            json.dump({
                "name": name,
                "version": "1.0",
                "count": len(data),
                "data": data
            }, f, indent=2, ensure_ascii=False)

        print(f"âœ… æ•°æ®é›†å·²åˆ›å»º: {dataset_path}")
        print(f"   åŒ…å« {len(data)} æ¡æ•°æ®")

    def load_dataset(self, name: str) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é›†"""
        dataset_path = self.datasets_dir / f"{name}.json"

        if not dataset_path.exists():
            raise FileNotFoundError(f"æ•°æ®é›†ä¸å­˜åœ¨: {name}")

        with open(dataset_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)

        print(f"âœ… æ•°æ®é›†å·²åŠ è½½: {name}")
        print(f"   ç‰ˆæœ¬: {dataset['version']}")
        print(f"   æ•°æ®é‡: {dataset['count']}")

        return dataset

    def list_datasets(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†"""
        datasets = [f.stem for f in self.datasets_dir.glob("*.json")]
        return datasets

    def merge_datasets(self, dataset_names: List[str], output_name: str):
        """åˆå¹¶å¤šä¸ªæ•°æ®é›†"""
        merged_data = []

        for name in dataset_names:
            dataset = self.load_dataset(name)
            merged_data.extend(dataset['data'])

        self.create_dataset(output_name, merged_data)
        print(f"âœ… å·²åˆå¹¶ {len(dataset_names)} ä¸ªæ•°æ®é›†")

    def split_dataset(self, name: str, train_ratio: float = 0.8):
        """åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
        dataset = self.load_dataset(name)
        data = dataset['data']

        # æ‰“ä¹±æ•°æ®
        import random
        random.shuffle(data)

        # åˆ†å‰²
        split_point = int(len(data) * train_ratio)
        train_data = data[:split_point]
        test_data = data[split_point:]

        # ä¿å­˜
        self.create_dataset(f"{name}_train", train_data)
        self.create_dataset(f"{name}_test", test_data)

        print(f"âœ… æ•°æ®é›†å·²åˆ†å‰²:")
        print(f"   è®­ç»ƒé›†: {len(train_data)} æ¡")
        print(f"   æµ‹è¯•é›†: {len(test_data)} æ¡")

# ä½¿ç”¨ç¤ºä¾‹
manager = DatasetManager()

# åˆ›å»ºæ•°æ®é›†
math_problems = [
    {
        "id": "math_001",
        "prompt": "è®¡ç®— 5 + 3",
        "expected_output": "8",
        "assertions": [{"type": "contains", "expected": "8"}],
        "difficulty": "easy"
    },
    {
        "id": "math_002",
        "prompt": "è®¡ç®— 12 Ã— 7",
        "expected_output": "84",
        "assertions": [{"type": "contains", "expected": "84"}],
        "difficulty": "easy"
    },
    {
        "id": "math_003",
        "prompt": "æ±‚è§£æ–¹ç¨‹ 2x + 5 = 13",
        "expected_output": "x = 4",
        "assertions": [
            {"type": "contains", "expected": "4"},
            {"type": "contains", "expected": "x"}
        ],
        "difficulty": "medium"
    }
]

manager.create_dataset("math_problems", math_problems)

# åŠ è½½æ•°æ®é›†
dataset = manager.load_dataset("math_problems")

# åˆ†å‰²æ•°æ®é›†
manager.split_dataset("math_problems", train_ratio=0.7)

# åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†
datasets = manager.list_datasets()
print(f"\næ‰€æœ‰æ•°æ®é›†: {datasets}")
```

---

## æœ€ä½³å®è·µ

### 1. æµ‹è¯•ç»„ç»‡

```python
# tests/test_agent_math.py

from tigerhill.storage.trace_store import TraceStore
from tigerhill.core.models import Task
from tigerhill.eval.assertions import run_assertions

class TestMathAgent:
    """æ•°å­¦ Agent æµ‹è¯•å¥—ä»¶"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•å‰çš„è®¾ç½®"""
        self.store = TraceStore(storage_path="./test_traces")
        self.agent = YourMathAgent()  # ä½ çš„ Agent å®ç°

    def teardown_method(self):
        """æ¯ä¸ªæµ‹è¯•åçš„æ¸…ç†"""
        pass

    def test_addition(self):
        """æµ‹è¯•åŠ æ³•"""
        task = Task(
            prompt="è®¡ç®— 5 + 3",
            assertions=[{"type": "contains", "expected": "8"}]
        )

        trace_id = self.store.start_trace(agent_name="math_agent", task_id="test_addition")
        output = self.agent.run(task.prompt)
        self.store.end_trace(trace_id)

        results = run_assertions(output, task.assertions)
        assert all(r["ok"] for r in results), "åŠ æ³•æµ‹è¯•å¤±è´¥"

    def test_multiplication(self):
        """æµ‹è¯•ä¹˜æ³•"""
        task = Task(
            prompt="è®¡ç®— 4 Ã— 7",
            assertions=[{"type": "contains", "expected": "28"}]
        )

        trace_id = self.store.start_trace(agent_name="math_agent", task_id="test_multiplication")
        output = self.agent.run(task.prompt)
        self.store.end_trace(trace_id)

        results = run_assertions(output, task.assertions)
        assert all(r["ok"] for r in results), "ä¹˜æ³•æµ‹è¯•å¤±è´¥"

    def test_equation_solving(self):
        """æµ‹è¯•æ–¹ç¨‹æ±‚è§£"""
        task = Task(
            prompt="æ±‚è§£ 2x + 5 = 13",
            assertions=[
                {"type": "contains", "expected": "4"},
                {"type": "regex", "pattern": r"x\s*=\s*4"}
            ]
        )

        trace_id = self.store.start_trace(agent_name="math_agent", task_id="test_equation")
        output = self.agent.run(task.prompt)
        self.store.end_trace(trace_id)

        results = run_assertions(output, task.assertions)
        assert all(r["ok"] for r in results), "æ–¹ç¨‹æ±‚è§£å¤±è´¥"
```

---

### 2. é…ç½®ç®¡ç†

```python
# config.py

from dataclasses import dataclass
from typing import Optional

@dataclass
class TigerHillConfig:
    """TigerHill é…ç½®"""

    # è¿½è¸ªé…ç½®
    trace_storage_path: str = "./traces"
    auto_save_traces: bool = True

    # AgentBay é…ç½®
    agentbay_api_key: Optional[str] = None
    agentbay_default_env: str = "codespace"

    # è¯„ä¼°é…ç½®
    enable_llm_judge: bool = False
    llm_judge_model: str = "gpt-4"

    # è°ƒè¯•é…ç½®
    debug_mode: bool = False
    verbose_logging: bool = True

    # æ€§èƒ½é…ç½®
    max_retries: int = 3
    timeout_seconds: int = 30

# ä½¿ç”¨é…ç½®
config = TigerHillConfig(
    trace_storage_path="./my_traces",
    debug_mode=True
)
```

---

### 3. æ—¥å¿—è®°å½•

```python
import logging
from tigerhill.storage.trace_store import TraceStore

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tigerhill.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger('tigerhill')

def test_with_logging():
    """å¸¦æ—¥å¿—è®°å½•çš„æµ‹è¯•"""
    logger.info("å¼€å§‹æµ‹è¯•")

    store = TraceStore()
    trace_id = store.start_trace(agent_name="test_agent")

    try:
        logger.info("æ‰§è¡Œ Agent")
        # ... Agent æ‰§è¡Œ
        logger.info("Agent æ‰§è¡ŒæˆåŠŸ")
    except Exception as e:
        logger.error(f"Agent æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        raise
    finally:
        store.end_trace(trace_id)
        logger.info(f"æµ‹è¯•å®Œæˆï¼Œè¿½è¸ª ID: {trace_id}")
```

---

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. AgentBay API Key é”™è¯¯

**é—®é¢˜**: `NOT_LOGIN` é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥ API key æ˜¯å¦è®¾ç½®
echo $AGENTBAY_API_KEY

# 2. ç¡®ä¿ key æ ¼å¼æ­£ç¡®ï¼ˆåº”è¯¥ä»¥ akm- å¼€å¤´ï¼‰
export AGENTBAY_API_KEY=akm-your-key-here

# 3. éªŒè¯ key æ˜¯å¦æœ‰æ•ˆ
python -c "from tigerhill.agentbay.client import AgentBayClient; AgentBayClient()"
```

#### 2. è¿½è¸ªæ–‡ä»¶æ‰¾ä¸åˆ°

**é—®é¢˜**: æ— æ³•åŠ è½½è¿½è¸ªæ–‡ä»¶

**è§£å†³æ–¹æ¡ˆ**:
```python
from tigerhill.storage.trace_store import TraceStore

# æ£€æŸ¥å­˜å‚¨è·¯å¾„
store = TraceStore(storage_path="./traces")
print(f"å­˜å‚¨è·¯å¾„: {store.storage_path}")
print(f"è·¯å¾„å­˜åœ¨: {store.storage_path.exists()}")

# åˆ—å‡ºæ‰€æœ‰è¿½è¸ª
traces = store.get_all_traces()
print(f"è¿½è¸ªæ•°é‡: {len(traces)}")
```

#### 3. æ–­è¨€å¤±è´¥

**é—®é¢˜**: æ–­è¨€æ€»æ˜¯å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```python
from tigerhill.eval.assertions import run_assertions

output = "å®é™…è¾“å‡ºå†…å®¹"
assertions = [{"type": "contains", "expected": "æœŸæœ›å†…å®¹"}]

results = run_assertions(output, assertions)

# è¯¦ç»†æŸ¥çœ‹å¤±è´¥åŸå› 
for result in results:
    if not result["ok"]:
        print(f"æ–­è¨€ç±»å‹: {result['type']}")
        print(f"æœŸæœ›å€¼: {result['expected']}")
        print(f"å®é™…å€¼: {result['actual']}")
        print(f"å¤±è´¥åŸå› : {result['message']}")
```

---

## é™„å½•

### A. å®Œæ•´ç¤ºä¾‹é¡¹ç›®

```
my_agent_project/
â”œâ”€â”€ agent/
â”‚   â””â”€â”€ my_agent.py          # ä½ çš„ Agent å®ç°
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_basic.py        # åŸºç¡€æµ‹è¯•
â”‚   â”œâ”€â”€ test_advanced.py     # é«˜çº§æµ‹è¯•
â”‚   â””â”€â”€ test_integration.py  # é›†æˆæµ‹è¯•
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ train.json          # è®­ç»ƒæ•°æ®é›†
â”‚   â””â”€â”€ test.json           # æµ‹è¯•æ•°æ®é›†
â”œâ”€â”€ traces/
â”‚   â””â”€â”€ (è‡ªåŠ¨ç”Ÿæˆ)          # è¿½è¸ªæ–‡ä»¶
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ (è‡ªåŠ¨ç”Ÿæˆ)          # æµ‹è¯•æŠ¥å‘Š
â”œâ”€â”€ config.py               # é…ç½®æ–‡ä»¶
â””â”€â”€ run_tests.py            # æµ‹è¯•è¿è¡Œè„šæœ¬
```

### B. å‘½ä»¤è¡Œå·¥å…·

```bash
# åˆ›å»ºä¸€ä¸ªå‘½ä»¤è¡Œè„šæœ¬ (cli.py)
from tigerhill.storage.trace_store import TraceStore
from tigerhill.eval.assertions import run_assertions
import click

@click.group()
def cli():
    """TigerHill CLI å·¥å…·"""
    pass

@cli.command()
@click.argument('trace_id')
def show_trace(trace_id):
    """æ˜¾ç¤ºè¿½è¸ªè¯¦æƒ…"""
    store = TraceStore()
    trace = store.get_trace(trace_id)

    if trace:
        summary = store.get_summary(trace_id)
        click.echo(f"è¿½è¸ª ID: {trace_id}")
        click.echo(f"Agent: {summary['agent_name']}")
        click.echo(f"è€—æ—¶: {summary['duration_seconds']:.2f} ç§’")
        click.echo(f"äº‹ä»¶æ•°: {summary['total_events']}")
    else:
        click.echo(f"è¿½è¸ªä¸å­˜åœ¨: {trace_id}")

@cli.command()
def list_traces():
    """åˆ—å‡ºæ‰€æœ‰è¿½è¸ª"""
    store = TraceStore()
    traces = store.get_all_traces()

    click.echo(f"æ€»è¿½è¸ªæ•°: {len(traces)}")
    for trace in traces[:10]:  # æ˜¾ç¤ºæœ€è¿‘ 10 ä¸ª
        click.echo(f"- {trace.trace_id}: {trace.agent_name}")

if __name__ == '__main__':
    cli()
```

ä½¿ç”¨ï¼š
```bash
python cli.py list-traces
python cli.py show-trace <trace_id>
```

---

## æ€»ç»“

TigerHill æä¾›äº†å®Œæ•´çš„ Agent æµ‹è¯•ã€è¯„ä¼°å’Œè°ƒè¯•èƒ½åŠ›ï¼š

âœ… **æµ‹è¯•**: å•æ¬¡ã€æ‰¹é‡ã€å›å½’æµ‹è¯•
âœ… **è¯„ä¼°**: æ–­è¨€ã€å¯¹æ¯”ã€è‡ªå®šä¹‰è¯„ä¼°å™¨
âœ… **è°ƒè¯•**: è¯¦ç»†è¿½è¸ªã€æ€§èƒ½åˆ†æã€é”™è¯¯è¿½è¸ª
âœ… **é›†æˆ**: AgentBay äº‘ç«¯ç¯å¢ƒæ”¯æŒ
âœ… **ç®¡ç†**: æ•°æ®é›†ç®¡ç†ã€é…ç½®ç®¡ç†

**å¼€å§‹ä½¿ç”¨**:
1. å®‰è£… TigerHill
2. å®šä¹‰æµ‹è¯•ä»»åŠ¡
3. è¿è¡Œå¹¶è¿½è¸ª Agent
4. è¯„ä¼°å’Œåˆ†æç»“æœ
5. æŒç»­ä¼˜åŒ–

**è·å–å¸®åŠ©**:
- æ–‡æ¡£: `REFACTORING_SUMMARY.md`
- ç¤ºä¾‹: `examples/basic_usage.py`
- æµ‹è¯•: `tests/test_integration.py`

**ç¥ä½ çš„ Agent å¼€å‘é¡ºåˆ©ï¼** ğŸ¯
