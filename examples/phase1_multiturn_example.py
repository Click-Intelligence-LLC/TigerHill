"""
Phase 1 å¢å¼ºåŠŸèƒ½æ¼”ç¤º

æ¼”ç¤ºç³»ç»ŸPromptæ•è·å’Œå¤šè½®å¯¹è¯è¿½è¸ªåŠŸèƒ½
"""

import json
from tigerhill.observer import PromptCapture
from tigerhill.observer.conversation_models import SystemPromptExtractor

def demonstrate_system_prompt_extraction():
    """æ¼”ç¤ºä»å¤šç§æ ¼å¼æå–ç³»ç»Ÿprompt"""
    print("=" * 80)
    print("æ¼”ç¤º 1ï¼šç³»ç»ŸPromptæå–ï¼ˆæ”¯æŒå¤šç§agentæ ¼å¼ï¼‰")
    print("=" * 80)

    # 1. Geminiæ ¼å¼
    print("\n1ï¸âƒ£ Geminiæ ¼å¼ (system_instruction):")
    gemini_kwargs = {
        'system_instruction': "You are a helpful AI assistant specialized in Python programming."
    }
    system_prompt = SystemPromptExtractor.extract_from_kwargs(gemini_kwargs)
    print(f"   æå–ç»“æœ: {system_prompt}")

    # 2. OpenAIæ ¼å¼
    print("\n2ï¸âƒ£  OpenAIæ ¼å¼ (messagesæ•°ç»„ä¸­çš„system role):")
    openai_kwargs = {
        'messages': [
            {'role': 'system', 'content': 'You are an expert code reviewer.'},
            {'role': 'user', 'content': 'Review my code'}
        ]
    }
    system_prompt = SystemPromptExtractor.extract_from_kwargs(openai_kwargs)
    print(f"   æå–ç»“æœ: {system_prompt}")

    # 3. Anthropicæ ¼å¼
    print("\n3ï¸âƒ£ Anthropicæ ¼å¼ (systemå‚æ•°):")
    anthropic_kwargs = {
        'system': 'You are Claude, a helpful AI assistant.'
    }
    system_prompt = SystemPromptExtractor.extract_from_kwargs(anthropic_kwargs)
    print(f"   æå–ç»“æœ: {system_prompt}")

    print("\nâœ… ç³»ç»ŸPromptæå–æ¼”ç¤ºå®Œæˆï¼")


def demonstrate_multiturn_conversation():
    """æ¼”ç¤ºå¤šè½®å¯¹è¯è¿½è¸ª"""
    print("\n" + "=" * 80)
    print("æ¼”ç¤º 2ï¼šå¤šè½®å¯¹è¯è¿½è¸ª")
    print("=" * 80)

    # åˆ›å»ºæ•è·å™¨
    capture = PromptCapture(storage_path="./prompt_captures/phase1_demo")
    capture_id = capture.start_capture("demo_agent")

    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    conversation_id = "demo_conversation_001"

    print(f"\nğŸ“ å¯¹è¯ID: {conversation_id}")
    print("-" * 80)

    # Turn 1
    print("\nğŸ”µ Turn 1:")
    print("   User: ä»€ä¹ˆæ˜¯Pythonï¼Ÿ")
    request_id_1 = capture.capture_request(
        capture_id,
        {
            "model": "gemini-2.0-flash-exp",
            "prompt": "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹æ•™å­¦åŠ©æ‰‹ï¼Œæ“…é•¿è§£é‡Šç¼–ç¨‹æ¦‚å¿µã€‚"
        },
        conversation_id=conversation_id,
        turn_number=1
    )

    capture.capture_response(
        capture_id,
        {
            "text": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´æ˜“è¯»çš„è¯­æ³•è€Œé—»åã€‚",
            "usage": {
                "prompt_tokens": 25,
                "completion_tokens": 30,
                "total_tokens": 55
            }
        },
        request_id=request_id_1
    )
    print("   Assistant: Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´æ˜“è¯»çš„è¯­æ³•è€Œé—»åã€‚")
    print("   ğŸ“Š Tokens: 55")

    # Turn 2
    print("\nğŸ”µ Turn 2:")
    print("   User: å®ƒæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ")
    request_id_2 = capture.capture_request(
        capture_id,
        {
            "model": "gemini-2.0-flash-exp",
            "prompt": "å®ƒæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
        },
        conversation_id=conversation_id,
        turn_number=2
    )

    capture.capture_response(
        capture_id,
        {
            "text": "Pythonçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š1ï¼‰æ˜“å­¦æ˜“ç”¨ 2ï¼‰ä¸°å¯Œçš„æ ‡å‡†åº“ 3ï¼‰å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒ 4ï¼‰è·¨å¹³å°å…¼å®¹æ€§å¥½ã€‚",
            "usage": {
                "prompt_tokens": 30,
                "completion_tokens": 45,
                "total_tokens": 75
            }
        },
        request_id=request_id_2
    )
    print("   Assistant: Pythonçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š1ï¼‰æ˜“å­¦æ˜“ç”¨ 2ï¼‰ä¸°å¯Œçš„æ ‡å‡†åº“ 3ï¼‰å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒ...")
    print("   ğŸ“Š Tokens: 75")

    # Turn 3
    print("\nğŸ”µ Turn 3:")
    print("   User: é€‚åˆåˆå­¦è€…å—ï¼Ÿ")
    request_id_3 = capture.capture_request(
        capture_id,
        {
            "model": "gemini-2.0-flash-exp",
            "prompt": "é€‚åˆåˆå­¦è€…å—ï¼Ÿ"
        },
        conversation_id=conversation_id,
        turn_number=3
    )

    capture.capture_response(
        capture_id,
        {
            "text": "éå¸¸é€‚åˆï¼Pythonçš„è¯­æ³•æ¥è¿‘è‡ªç„¶è¯­è¨€ï¼Œæ˜¯æœ€é€‚åˆç¼–ç¨‹åˆå­¦è€…çš„è¯­è¨€ä¹‹ä¸€ã€‚",
            "usage": {
                "prompt_tokens": 20,
                "completion_tokens": 35,
                "total_tokens": 55
            }
        },
        request_id=request_id_3
    )
    print("   Assistant: éå¸¸é€‚åˆï¼Pythonçš„è¯­æ³•æ¥è¿‘è‡ªç„¶è¯­è¨€ï¼Œæ˜¯æœ€é€‚åˆç¼–ç¨‹åˆå­¦è€…çš„è¯­è¨€ä¹‹ä¸€ã€‚")
    print("   ğŸ“Š Tokens: 55")

    print("\n" + "-" * 80)

    # è·å–å¯¹è¯å†å²
    conv_history = capture.get_conversation_history(conversation_id)

    print("\nğŸ“Š å¯¹è¯ç»Ÿè®¡:")
    print(f"   â€¢ æ€»è½®æ¬¡: {conv_history.total_turns}")
    print(f"   â€¢ æ¶ˆæ¯æ€»æ•°: {len(conv_history.messages)}")
    print(f"   â€¢ ç³»ç»Ÿprompt: {'âœ“ å·²è®¾ç½®' if conv_history.system_prompt else 'âœ— æœªè®¾ç½®'}")
    print(f"   â€¢ æ€»Tokenæ¶ˆè€—: {conv_history.total_tokens['total_tokens']}")

    print("\nğŸ“ æ¶ˆæ¯ç»“æ„:")
    for msg in conv_history.messages:
        role_icon = {
            'system': 'ğŸ”§',
            'user': 'ğŸ‘¤',
            'assistant': 'ğŸ¤–'
        }.get(msg.role, 'â€¢')

        content_preview = msg.content[:50] + "..." if len(msg.content) > 50 else msg.content
        print(f"   {role_icon} [{msg.role:10s}] Turn {msg.turn_number}: {content_preview}")

    # è·å–å¯¹è¯æ‘˜è¦
    print("\nğŸ“‹ å¯¹è¯æ‘˜è¦:")
    summary = capture.get_conversation_summary(conversation_id)
    print(f"   â€¢ ç³»ç»Ÿprompté¢„è§ˆ: {summary['system_prompt_preview'][:60]}...")
    print(f"   â€¢ å¯¹è¯æ—¶é•¿: {summary['duration']:.2f}ç§’")
    print(f"   â€¢ Tokenç»Ÿè®¡: {summary['total_tokens']}")

    print("\nğŸ’¾ å¯¼å‡ºå¯¹è¯å†å²...")
    export_path = "./prompt_captures/phase1_demo/conversation_demo.json"
    capture.export_conversation(conversation_id, export_path)
    print(f"   âœ… å·²ä¿å­˜åˆ°: {export_path}")

    # ç»“æŸæ•è·
    result = capture.end_capture(capture_id)
    print(f"\nâœ… å¤šè½®å¯¹è¯è¿½è¸ªæ¼”ç¤ºå®Œæˆï¼")
    print(f"   æ•è·äº† {result['statistics']['total_requests']} ä¸ªè¯·æ±‚")
    print(f"   æ•è·äº† {result['statistics']['total_responses']} ä¸ªå“åº”")


def demonstrate_conversation_structure():
    """æ¼”ç¤ºå¯¹è¯ç»“æ„æŸ¥è¯¢"""
    print("\n" + "=" * 80)
    print("æ¼”ç¤º 3ï¼šå¯¹è¯ç»“æ„æŸ¥è¯¢")
    print("=" * 80)

    capture = PromptCapture(storage_path="./prompt_captures/phase1_demo")
    capture_id = capture.start_capture("structure_demo")

    # åˆ›å»ºä¸¤ä¸ªä¸åŒçš„å¯¹è¯
    conversations = [
        ("conv_python_basics", [
            ("Turn 1", "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ", "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€"),
            ("Turn 2", "å¦‚ä½•å­¦ä¹ ï¼Ÿ", "ä»åŸºç¡€è¯­æ³•å¼€å§‹å­¦ä¹ ")
        ]),
        ("conv_data_structures", [
            ("Turn 1", "ä»€ä¹ˆæ˜¯åˆ—è¡¨ï¼Ÿ", "åˆ—è¡¨æ˜¯Pythonä¸­çš„ä¸€ç§æ•°æ®ç»“æ„"),
            ("Turn 2", "å¦‚ä½•ä½¿ç”¨ï¼Ÿ", "å¯ä»¥ç”¨[]åˆ›å»ºåˆ—è¡¨")
        ])
    ]

    for conv_id, turns in conversations:
        for turn_num, (_, user_msg, assistant_msg) in enumerate(turns, 1):
            req_id = capture.capture_request(
                capture_id,
                {"model": "test", "prompt": user_msg},
                conversation_id=conv_id,
                turn_number=turn_num
            )
            capture.capture_response(
                capture_id,
                {"text": assistant_msg},
                request_id=req_id
            )

    # åˆ—å‡ºæ‰€æœ‰å¯¹è¯
    print("\nğŸ“‹ å½“å‰ä¼šè¯ä¸­çš„å¯¹è¯åˆ—è¡¨:")
    conversations_list = capture.list_conversations()
    for idx, conv in enumerate(conversations_list, 1):
        print(f"\n{idx}. å¯¹è¯ID: {conv['conversation_id']}")
        print(f"   â€¢ Agent: {conv['agent_name']}")
        print(f"   â€¢ è½®æ¬¡æ•°: {conv['total_turns']}")
        print(f"   â€¢ æ¶ˆæ¯æ•°: {conv['message_count']}")
        print(f"   â€¢ å¼€å§‹æ—¶é—´: {conv['started_at']}")

    print("\nâœ… å¯¹è¯ç»“æ„æŸ¥è¯¢æ¼”ç¤ºå®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸš€" * 40)
    print("TigerHill Observer SDK - Phase 1 å¢å¼ºåŠŸèƒ½æ¼”ç¤º")
    print("ğŸš€" * 40)

    # æ¼”ç¤º1ï¼šç³»ç»Ÿpromptæå–
    demonstrate_system_prompt_extraction()

    # æ¼”ç¤º2ï¼šå¤šè½®å¯¹è¯è¿½è¸ª
    demonstrate_multiturn_conversation()

    # æ¼”ç¤º3ï¼šå¯¹è¯ç»“æ„æŸ¥è¯¢
    demonstrate_conversation_structure()

    print("\n" + "=" * 80)
    print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)

    print("\nğŸ’¡ Phase 1 å¢å¼ºåŠŸèƒ½æ€»ç»“:")
    print("   âœ… æ”¯æŒä»Geminiã€OpenAIã€Anthropicç­‰å¤šç§æ ¼å¼æå–ç³»ç»Ÿprompt")
    print("   âœ… ç»“æ„åŒ–å¯¹è¯å†å²è¿½è¸ªï¼ˆåŒ…å«è§’è‰²ã€turn_numberç­‰ï¼‰")
    print("   âœ… è‡ªåŠ¨conversation_idç”Ÿæˆå’Œç®¡ç†")
    print("   âœ… å®Œæ•´çš„å¯¹è¯ç»Ÿè®¡å’Œæ‘˜è¦åŠŸèƒ½")
    print("   âœ… å¯¹è¯å†å²å¯¼å‡ºåŠŸèƒ½")

    print("\nğŸ¯ æ”¯æŒçš„ä½¿ç”¨åœºæ™¯:")
    print("   â€¢ gemini-cli å¤šè½®å¯¹è¯è¿½è¸ª")
    print("   â€¢ å…¶ä»–CLI agentçš„å¯¹è¯æ•è·")
    print("   â€¢ LLM APIçš„è¯·æ±‚/å“åº”å®Œæ•´è®°å½•")
    print("   â€¢ ç³»ç»Ÿpromptè´¨é‡åˆ†æ")
    print("   â€¢ å¤šè½®å¯¹è¯coherenceåˆ†æ")

    print("\nğŸ“š æŸ¥çœ‹æ›´å¤š:")
    print("   â€¢ æµ‹è¯•æ–‡ä»¶: tests/test_observer_phase1_enhancements.py")
    print("   â€¢ æ–‡æ¡£: OBSERVER_SDK_CAPABILITIES_ANALYSIS.md")
    print()


if __name__ == "__main__":
    main()
