# LLM Multi-turn Conversation Testing Template
# Tests multi-turn conversations with context tracking

metadata:
  name: "llm-multi-turn"
  display_name: "LLM Multi-turn Conversation Testing"
  description: "Test multi-turn conversations with context and state tracking"
  category: "llm"
  version: "1.0.0"
  author: "TigerHill"
  tags: ["llm", "multi-turn", "conversation", "context"]

parameters:
  - name: "agent_name"
    display_name: "Agent Name"
    description: "Name for the test agent"
    type: "string"
    required: true
    default: "my-conversation-agent"
    validation:
      pattern: "^[a-zA-Z0-9_-]+$"

  - name: "model_name"
    display_name: "Model Name"
    description: "LLM model to test"
    type: "choice"
    required: true
    default: "gpt-4"
    choices: ["gpt-4", "gpt-3.5-turbo", "claude-3-opus", "claude-3-sonnet", "gemini-pro"]

  - name: "num_turns"
    display_name: "Number of Turns"
    description: "Number of conversation turns to test"
    type: "integer"
    required: true
    default: 3
    validation:
      min: 2
      max: 10

  - name: "validate_context"
    display_name: "Validate Context"
    description: "Validate that model maintains context across turns"
    type: "boolean"
    required: true
    default: true

dependencies:
  pip:
    - "pytest>=7.4.0"
    - "openai>=1.0.0"

files:
  - path: "test_{{agent_name}}.py"
    template: "main_script"
  - path: "requirements.txt"
    template: "requirements"
  - path: "README.md"
    template: "readme"

templates:
  main_script: |
    #!/usr/bin/env python3
    """
    {{ metadata.display_name }} - Generated by TigerHill

    {{ metadata.description }}
    """

    import pytest
    from tigerhill.observer.session_tracker import SessionTracker
    from tigerhill.storage.trace_store import TraceStore, EventType


    class Test{{ agent_name | camel_case }}:
        """{{ metadata.display_name }} Test Suite"""

        @pytest.fixture
        def session_tracker(self):
            """Create session tracker"""
            return SessionTracker()

        @pytest.fixture
        def trace_store(self, tmp_path):
            """Create trace store for recording"""
            return TraceStore(storage_path=str(tmp_path))

        def test_{{ agent_name | snake_case }}(self, session_tracker, trace_store):
            """Test {{ metadata.display_name }}"""

            # Start trace
            trace_id = trace_store.start_trace(
                agent_name="{{ agent_name }}",
                task_id="multi-turn-test-{{ agent_name }}",
                metadata={
                    "model": "{{ model_name }}",
                    "num_turns": {{ num_turns }}
                }
            )

            try:
                # Initialize conversation
                session_id = session_tracker.start_session(
                    agent_name="{{ agent_name }}",
                    metadata={"trace_id": trace_id}
                )

                conversation_history = []

                # Simulate {{ num_turns }} conversation turns
                for turn in range({{ num_turns }}):
                    # User message
                    user_message = f"Turn {turn + 1} message from user"

                    trace_store.write_event(
                        {
                            "type": "prompt",
                            "content": user_message,
                            "model": "{{ model_name }}",
                            "turn": turn + 1,
                            "session_id": session_id
                        },
                        trace_id=trace_id,
                        event_type=EventType.PROMPT
                    )

                    conversation_history.append({
                        "role": "user",
                        "content": user_message
                    })

                    # TODO: Replace with actual LLM API call
                    # For multi-turn, pass conversation_history as context
                    assistant_message = f"Turn {turn + 1} response from assistant"

                    trace_store.write_event(
                        {
                            "type": "model_response",
                            "content": assistant_message,
                            "model": "{{ model_name }}",
                            "turn": turn + 1,
                            "session_id": session_id
                        },
                        trace_id=trace_id,
                        event_type=EventType.MODEL_RESPONSE
                    )

                    conversation_history.append({
                        "role": "assistant",
                        "content": assistant_message
                    })

                    # Track turn
                    session_tracker.record_turn(
                        session_id=session_id,
                        user_message=user_message,
                        assistant_message=assistant_message,
                        turn_number=turn + 1
                    )

                {% if validate_context -%}
                # Validate context maintenance
                session_data = session_tracker.get_session(session_id)
                assert len(session_data["turns"]) == {{ num_turns }}, \
                    f"Expected {{ num_turns }} turns, got {len(session_data['turns'])}"

                # Validate conversation flow
                assert len(conversation_history) == {{ num_turns }} * 2, \
                    "Conversation history should have user + assistant messages"
                {% endif -%}

                # End session
                session_tracker.end_session(session_id)

                print(f"âœ… Test passed: {{ agent_name }}")
                print(f"Completed {len(conversation_history) // 2} conversation turns")

            except Exception as e:
                trace_store.write_event(
                    {
                        "type": "error",
                        "error_message": str(e),
                        "error_type": type(e).__name__
                    },
                    trace_id=trace_id,
                    event_type=EventType.ERROR
                )
                raise

            finally:
                # End trace
                trace_store.end_trace(trace_id)


    if __name__ == "__main__":
        pytest.main([__file__, "-v", "-s"])

  requirements: |
    # Generated by TigerHill Template Library
    # Template: {{ metadata.name }}
    {% for dep in dependencies.pip -%}
    {{ dep }}
    {% endfor %}

  readme: |
    # {{ metadata.display_name }}

    {{ metadata.description }}

    ## Generated Configuration

    - **Agent Name**: `{{ agent_name }}`
    - **Model**: `{{ model_name }}`
    - **Conversation Turns**: `{{ num_turns }}`
    - **Context Validation**: `{{ 'Enabled' if validate_context else 'Disabled' }}`

    ## Installation

    ```bash
    pip install -r requirements.txt
    ```

    ## Usage

    Run the test:

    ```bash
    pytest test_{{ agent_name }}.py -v
    ```

    ## Customization

    Replace the placeholder with actual LLM API calls that maintain conversation history.

    ## Documentation

    For more information, see:
    - [TigerHill Documentation](https://github.com/yourusername/tigerhill)
    - [Session Tracking Guide](https://github.com/yourusername/tigerhill/docs/session_tracking.md)
