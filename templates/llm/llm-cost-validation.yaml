# LLM Cost Validation Template
# Tests LLM interactions with cost and token tracking

metadata:
  name: "llm-cost-validation"
  display_name: "LLM Cost & Token Validation"
  description: "Test LLM with cost tracking, token limits, and budget validation"
  category: "llm"
  version: "1.0.0"
  author: "TigerHill"
  tags: ["llm", "cost", "tokens", "budget", "optimization"]

parameters:
  - name: "agent_name"
    display_name: "Agent Name"
    description: "Name for the test agent"
    type: "string"
    required: true
    default: "my-cost-agent"
    validation:
      pattern: "^[a-zA-Z0-9_-]+$"

  - name: "model_name"
    display_name: "Model Name"
    description: "LLM model to test"
    type: "choice"
    required: true
    default: "gpt-3.5-turbo"
    choices: ["gpt-4", "gpt-3.5-turbo", "claude-3-opus", "claude-3-sonnet", "gemini-pro"]

  - name: "max_budget_usd"
    display_name: "Max Budget (USD)"
    description: "Maximum allowed cost per test run"
    type: "float"
    required: true
    default: 0.10
    validation:
      min: 0.01
      max: 10.0

  - name: "max_tokens_per_call"
    display_name: "Max Tokens Per Call"
    description: "Maximum tokens allowed per LLM call"
    type: "integer"
    required: true
    default: 500
    validation:
      min: 50
      max: 10000

dependencies:
  pip:
    - "pytest>=7.4.0"

files:
  - path: "test_{{agent_name}}.py"
    template: "main_script"
  - path: "requirements.txt"
    template: "requirements"
  - path: "README.md"
    template: "readme"

templates:
  main_script: |
    #!/usr/bin/env python3
    """
    {{ metadata.display_name }} - Generated by TigerHill

    {{ metadata.description }}
    """

    import pytest
    from tigerhill.storage.sqlite_trace_store import SQLiteTraceStore
    from tigerhill.storage.trace_store import EventType


    class Test{{ agent_name | camel_case }}:
        """{{ metadata.display_name }} Test Suite"""

        @pytest.fixture
        def trace_store(self, tmp_path):
            """Create trace store for recording"""
            db_path = str(tmp_path / "test.db")
            return SQLiteTraceStore(db_path=db_path, auto_init=True)

        def test_{{ agent_name | snake_case }}(self, trace_store):
            """Test {{ metadata.display_name }}"""

            # Start trace
            trace_id = trace_store.start_trace(
                agent_name="{{ agent_name }}",
                task_id="cost-test-{{ agent_name }}",
                metadata={
                    "model": "{{ model_name }}",
                    "max_budget": {{ max_budget_usd }},
                    "max_tokens_per_call": {{ max_tokens_per_call }}
                }
            )

            total_cost = 0.0
            total_tokens = 0

            try:
                # Simulate LLM call with cost tracking
                prompt_tokens = 100
                completion_tokens = 200

                # Calculate costs (example rates)
                {% if model_name == "gpt-4" %}
                prompt_cost = prompt_tokens * 0.00003  # $0.03 per 1K tokens
                completion_cost = completion_tokens * 0.00006  # $0.06 per 1K tokens
                {% elif model_name == "gpt-3.5-turbo" %}
                prompt_cost = prompt_tokens * 0.0000015  # $0.0015 per 1K tokens
                completion_cost = completion_tokens * 0.000002  # $0.002 per 1K tokens
                {% else %}
                prompt_cost = prompt_tokens * 0.00001
                completion_cost = completion_tokens * 0.00003
                {% endif %}

                call_cost = prompt_cost + completion_cost
                call_tokens = prompt_tokens + completion_tokens

                # Record prompt
                trace_store.write_event(
                    {
                        "type": "prompt",
                        "content": "Test prompt for cost validation",
                        "model": "{{ model_name }}",
                        "total_tokens": prompt_tokens,
                        "cost_usd": prompt_cost
                    },
                    trace_id=trace_id,
                    event_type=EventType.PROMPT
                )

                # Record response
                trace_store.write_event(
                    {
                        "type": "model_response",
                        "content": "Test response",
                        "model": "{{ model_name }}",
                        "total_tokens": completion_tokens,
                        "cost_usd": completion_cost
                    },
                    trace_id=trace_id,
                    event_type=EventType.MODEL_RESPONSE
                )

                total_cost += call_cost
                total_tokens += call_tokens

                # Validate budget
                assert total_cost <= {{ max_budget_usd }}, \
                    f"Cost ${total_cost:.4f} exceeds budget ${{ max_budget_usd }}"

                # Validate token limit
                assert call_tokens <= {{ max_tokens_per_call }}, \
                    f"Tokens {call_tokens} exceeds limit {{ max_tokens_per_call }}"

                # Log cost summary
                trace_store.write_event(
                    {
                        "type": "cost_summary",
                        "total_cost_usd": total_cost,
                        "total_tokens": total_tokens,
                        "budget_remaining": {{ max_budget_usd }} - total_cost,
                        "budget_utilized_pct": (total_cost / {{ max_budget_usd }}) * 100
                    },
                    trace_id=trace_id
                )

                print(f"âœ… Test passed: {{ agent_name }}")
                print(f"Total Cost: ${total_cost:.4f} / ${{ max_budget_usd }}")
                print(f"Total Tokens: {total_tokens}")
                budget_remaining = {{ max_budget_usd }} - total_cost
                print(f"Budget Remaining: ${budget_remaining:.4f}")

            except Exception as e:
                trace_store.write_event(
                    {
                        "type": "error",
                        "error_message": str(e),
                        "error_type": type(e).__name__
                    },
                    trace_id=trace_id,
                    event_type=EventType.ERROR
                )
                raise

            finally:
                # End trace
                trace_store.end_trace(trace_id)

                # Get final statistics
                summary = trace_store.get_summary(trace_id)
                print(f"\nFinal Statistics:")
                print(f"  Total Cost: ${summary.get('total_cost_usd', 0):.4f}")
                print(f"  Total Tokens: {summary.get('total_tokens', 0)}")


    if __name__ == "__main__":
        pytest.main([__file__, "-v", "-s"])

  requirements: |
    # Generated by TigerHill Template Library
    # Template: {{ metadata.name }}
    {% for dep in dependencies.pip -%}
    {{ dep }}
    {% endfor %}

  readme: |
    # {{ metadata.display_name }}

    {{ metadata.description }}

    ## Generated Configuration

    - **Agent Name**: `{{ agent_name }}`
    - **Model**: `{{ model_name }}`
    - **Max Budget**: `${{ max_budget_usd }}`
    - **Max Tokens Per Call**: `{{ max_tokens_per_call }}`

    ## Cost Tracking

    This test tracks:
    - Token usage (prompt + completion)
    - Cost per call
    - Total cost vs. budget
    - Budget utilization percentage

    ## Installation

    ```bash
    pip install -r requirements.txt
    ```

    ## Usage

    Run the test:

    ```bash
    pytest test_{{ agent_name }}.py -v -s
    ```

    ## View Cost Analysis

    View detailed cost breakdown in the Dashboard:

    ```bash
    PYTHONPATH=. streamlit run tigerhill/web/dashboard/app.py
    ```

    ## Documentation

    For more information, see:
    - [TigerHill Documentation](https://github.com/yourusername/tigerhill)
    - [Cost Optimization Guide](https://github.com/yourusername/tigerhill/docs/cost_optimization.md)
