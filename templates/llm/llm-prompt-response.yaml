# LLM Prompt-Response Testing Template
# Tests basic LLM prompt and response validation

metadata:
  name: "llm-prompt-response"
  display_name: "LLM Prompt-Response Testing"
  description: "Test LLM prompt and response with quality validation"
  category: "llm"
  version: "1.0.0"
  author: "TigerHill"
  tags: ["llm", "prompt", "response", "quality"]

parameters:
  - name: "agent_name"
    display_name: "Agent Name"
    description: "Name for the test agent"
    type: "string"
    required: true
    default: "my-llm-agent"
    validation:
      pattern: "^[a-zA-Z0-9_-]+$"

  - name: "model_name"
    display_name: "Model Name"
    description: "LLM model to test"
    type: "choice"
    required: true
    default: "gpt-4"
    choices: ["gpt-4", "gpt-3.5-turbo", "claude-3-opus", "claude-3-sonnet", "gemini-pro"]

  - name: "prompt"
    display_name: "Test Prompt"
    description: "The prompt to send to the LLM"
    type: "string"
    required: true

  - name: "max_tokens"
    display_name: "Max Tokens"
    description: "Maximum tokens in response"
    type: "integer"
    required: true
    default: 1000
    validation:
      min: 1
      max: 100000

  - name: "temperature"
    display_name: "Temperature"
    description: "Sampling temperature (0-2)"
    type: "float"
    required: true
    default: 0.7
    validation:
      min: 0
      max: 2

  - name: "validate_quality"
    display_name: "Validate Quality"
    description: "Run quality validation on response"
    type: "boolean"
    required: true
    default: true

  - name: "expected_keywords"
    display_name: "Expected Keywords"
    description: "Comma-separated keywords that should appear in response"
    type: "string"
    required: false
    default: ""

dependencies:
  pip:
    - "pytest>=7.4.0"
    - "openai>=1.0.0"
    - "anthropic>=0.18.0"

files:
  - path: "test_{{agent_name}}.py"
    template: "main_script"
  - path: "requirements.txt"
    template: "requirements"
  - path: "README.md"
    template: "readme"

templates:
  main_script: |
    #!/usr/bin/env python3
    """
    {{ metadata.display_name }} - Generated by TigerHill

    {{ metadata.description }}
    """

    import pytest
    from tigerhill.observer.prompt_capture import PromptCapture
    from tigerhill.observer.prompt_analyzer import PromptAnalyzer
    from tigerhill.assertions import assert_response_contains, assert_response_length
    from tigerhill.storage.trace_store import TraceStore, EventType


    class Test{{ agent_name | camel_case }}:
        """{{ metadata.display_name }} Test Suite"""

        @pytest.fixture
        def capture(self):
            """Create prompt capture"""
            return PromptCapture(capture_path="./prompt_captures")

        @pytest.fixture
        def trace_store(self, tmp_path):
            """Create trace store for recording"""
            return TraceStore(storage_path=str(tmp_path))

        def test_{{ agent_name | snake_case }}(self, capture, trace_store):
            """Test {{ metadata.display_name }}"""

            # Start trace
            trace_id = trace_store.start_trace(
                agent_name="{{ agent_name }}",
                task_id="llm-test-{{ agent_name }}",
                metadata={
                    "model": "{{ model_name }}",
                    "temperature": {{ temperature }}
                }
            )

            try:
                # Record prompt
                prompt_text = """{{ prompt }}"""

                trace_store.write_event(
                    {
                        "type": "prompt",
                        "content": prompt_text,
                        "model": "{{ model_name }}",
                        "temperature": {{ temperature }},
                        "max_tokens": {{ max_tokens }}
                    },
                    trace_id=trace_id,
                    event_type=EventType.PROMPT
                )

                # TODO: Replace with actual LLM API call
                # For now, this is a placeholder showing the structure
                response_text = "This is a placeholder response. Replace with actual LLM call."

                # Record response
                trace_store.write_event(
                    {
                        "type": "model_response",
                        "content": response_text,
                        "model": "{{ model_name }}",
                        "finish_reason": "stop"
                    },
                    trace_id=trace_id,
                    event_type=EventType.MODEL_RESPONSE
                )

                {% if validate_quality %}
                # Validate response quality
                assert len(response_text) > 0, "Response should not be empty"
                assert len(response_text.split()) >= 5, "Response should have at least 5 words"

                {% if expected_keywords %}
                # Check for expected keywords
                keywords = [k.strip() for k in "{{ expected_keywords }}".split(",")]
                for keyword in keywords:
                    assert keyword.lower() in response_text.lower(), \
                        f"Expected keyword '{keyword}' not found in response"

                {% endif %}
                # Analyze prompt quality (optional)
                analyzer = PromptAnalyzer()
                analysis = analyzer.analyze({
                    "prompt": prompt_text,
                    "response": response_text,
                    "model": "{{ model_name }}"
                })

                trace_store.write_event(
                    {
                        "type": "quality_analysis",
                        "scores": analysis.get("scores", {}),
                        "recommendations": analysis.get("recommendations", [])
                    },
                    trace_id=trace_id
                )
                {% endif %}
                print(f"âœ… Test passed: {{ agent_name }}")
                print(f"Response length: {len(response_text)} chars")

            except Exception as e:
                trace_store.write_event(
                    {
                        "type": "error",
                        "error_message": str(e),
                        "error_type": type(e).__name__
                    },
                    trace_id=trace_id,
                    event_type=EventType.ERROR
                )
                raise

            finally:
                # End trace
                trace_store.end_trace(trace_id)


    if __name__ == "__main__":
        pytest.main([__file__, "-v", "-s"])

  requirements: |
    # Generated by TigerHill Template Library
    # Template: {{ metadata.name }}
    {% for dep in dependencies.pip -%}
    {{ dep }}
    {% endfor %}

  readme: |
    # {{ metadata.display_name }}

    {{ metadata.description }}

    ## Generated Configuration

    - **Agent Name**: `{{ agent_name }}`
    - **Model**: `{{ model_name }}`
    - **Temperature**: `{{ temperature }}`
    - **Max Tokens**: `{{ max_tokens }}`
    {% if expected_keywords -%}
    - **Expected Keywords**: `{{ expected_keywords }}`
    {% endif %}

    ## Installation

    ```bash
    pip install -r requirements.txt
    ```

    ## Setup

    Before running the test, you need to:

    1. **Add LLM API credentials** - Set environment variables:
       ```bash
       export OPENAI_API_KEY="your-openai-key"  # For GPT models
       export ANTHROPIC_API_KEY="your-anthropic-key"  # For Claude models
       ```

    2. **Replace the placeholder** - Update the test to make actual LLM API calls:
       ```python
       # Example for OpenAI
       import openai
       client = openai.OpenAI()
       response = client.chat.completions.create(
           model="{{ model_name }}",
           messages=[{"role": "user", "content": prompt_text}],
           temperature={{ temperature }},
           max_tokens={{ max_tokens }}
       )
       response_text = response.choices[0].message.content
       ```

    ## Usage

    Run the test:

    ```bash
    pytest test_{{ agent_name }}.py -v
    ```

    ## Customization

    You can modify the generated test script to add:

    - **Multiple Prompts**: Test different prompt variations
    - **Response Schema Validation**: Validate structured outputs
    - **Cost Tracking**: Monitor token usage and costs
    - **Latency Testing**: Measure response time

    ## Documentation

    For more information, see:
    - [TigerHill Documentation](https://github.com/yourusername/tigerhill)
    - [Observer SDK Guide](https://github.com/yourusername/tigerhill/docs/observer_sdk.md)
